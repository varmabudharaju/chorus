{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Chorus: Centralized Baseline + Zero-Shot\n\n**Run this notebook in parallel with `chorus_centralized_vs_federated.ipynb`.**\n\nThis notebook trains the centralized (gold standard) model on ALL 2K training examples\nand evaluates both the zero-shot baseline and centralized model. Results are saved to\n`DATA_DIR/centralized_results.json` for the federated notebook to load.\n\n| Setting | Value |\n|---------|-------|\n| Model | `Qwen/Qwen2.5-0.5B` |\n| Dataset | AG News — 2K train, 500 test |\n| LoRA | rank=16, alpha=32, 1 epoch |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q pyarrow==17.0.0\n!pip install -q --upgrade typing_extensions pydantic pydantic-core\n!pip install -q 'chorus[peft] @ git+https://github.com/varmabudharaju/chorus.git'\n!pip install -q scikit-learn"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch, gc, os, time, json, logging, random\nimport numpy as np\nfrom collections import Counter\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s [%(name)s] %(message)s', datefmt='%H:%M:%S')\n\n# ── Data directory: auto-detect Colab vs SageMaker ───────────────\nif os.path.exists(\"/content\"):\n    DATA_DIR = \"/content/chorus_exp\"        # Colab\nelse:\n    DATA_DIR = os.path.expanduser(\"~/chorus_exp\")  # SageMaker / local\nos.makedirs(DATA_DIR, exist_ok=True)\nprint(f\"Data directory: {DATA_DIR}\")\n\n# ── Must match federated notebook exactly ─────────────────────────\nMODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\nDATASET_SIZE = 2000\nTEST_SIZE = 500\nLORA_RANK = 16\nLORA_ALPHA = 32\nLEARNING_RATE = 3e-4\nNUM_EPOCHS = 1\nBATCH_SIZE = 8\nGRAD_ACCUM = 2\nMAX_SEQ_LEN = 128\nSEED = 42\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\nif device == \"cuda\":\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name} | VRAM: {gpu.total_memory / 1e9:.1f} GB\")\nelse:\n    print(\"WARNING: No GPU!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset, Dataset\n\nLABEL_NAMES = [\"world\", \"sports\", \"business\", \"scitech\"]\nLABEL_MAP = {0: \"world\", 1: \"sports\", 2: \"business\", 3: \"scitech\"}\n\nraw_train = load_dataset(\"ag_news\", split=\"train\").shuffle(seed=SEED)\nraw_test = load_dataset(\"ag_news\", split=\"test\").shuffle(seed=SEED)\n\ntrain_ds = raw_train.select(range(DATASET_SIZE))\ntest_ds = raw_test.select(range(TEST_SIZE))\n\nprint(f\"Train: {len(train_ds)} | Test: {len(test_ds)}\")\nprint(f\"Train labels: {dict(sorted(Counter(train_ds['label']).items()))}\")\nprint(f\"Test labels:  {dict(sorted(Counter(test_ds['label']).items()))}\")\n\ndef format_example(ex):\n    return f\"Sentence: {ex['text']}\\nCategory: {LABEL_MAP[ex['label']]}\"\n\ncentral_texts = [format_example(train_ds[j]) for j in range(len(train_ds))]\ncentral_ds = Dataset.from_dict({\"text\": central_texts})\ncentral_path = f\"{DATA_DIR}/centralized_train.json\"\ncentral_ds.to_json(central_path)\nprint(f\"Saved {len(train_ds)} training examples to {central_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model + Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "USE_BF16 = torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False\n",
    "DTYPE = torch.bfloat16 if USE_BF16 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "label_token_ids = {\n",
    "    0: tokenizer.encode(\" world\", add_special_tokens=False)[0],\n",
    "    1: tokenizer.encode(\" sports\", add_special_tokens=False)[0],\n",
    "    2: tokenizer.encode(\" business\", add_special_tokens=False)[0],\n",
    "    3: tokenizer.encode(\" scitech\", add_special_tokens=False)[0],\n",
    "}\n",
    "print(f\"Label tokens: {label_token_ids}\")\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, test_data, label):\n",
    "    model.eval()\n",
    "    preds, golds = [], []\n",
    "    for ex in test_data:\n",
    "        prompt = f\"Sentence: {ex['text']}\\nCategory:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LEN).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits[:, -1, :]\n",
    "        scores = {lbl: logits[0, tid].item() for lbl, tid in label_token_ids.items()}\n",
    "        preds.append(max(scores, key=scores.get))\n",
    "        golds.append(ex[\"label\"])\n",
    "    acc = accuracy_score(golds, preds)\n",
    "    report = classification_report(\n",
    "        golds, preds, target_names=LABEL_NAMES,\n",
    "        output_dict=True, zero_division=0\n",
    "    )\n",
    "    print(\n",
    "        f\"  [{label}] Accuracy: {acc:.1%}  |  F1: \"\n",
    "        f\"world={report['world']['f1-score']:.2f} \"\n",
    "        f\"sports={report['sports']['f1-score']:.2f} \"\n",
    "        f\"business={report['business']['f1-score']:.2f} \"\n",
    "        f\"scitech={report['scitech']['f1-score']:.2f}\"\n",
    "    )\n",
    "    return acc, report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Baseline (Zero-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(device)\n",
    "baseline_acc, baseline_report = evaluate_accuracy(base_model, test_ds, \"Baseline (zero-shot)\")\n",
    "del base_model; gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Centralized Training (Gold Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from chorus.client.trainer import LoRATrainer\nfrom peft import PeftModel\n\nprint(f\"Training centralized model on all {DATASET_SIZE} examples, {NUM_EPOCHS} epochs...\")\nt0 = time.time()\n\ncentralized_dir = f\"{DATA_DIR}/adapter_centralized\"\ntrainer = LoRATrainer(\n    base_model=MODEL_NAME,\n    dataset=central_path,\n    output_dir=centralized_dir,\n    lora_rank=LORA_RANK,\n    lora_alpha=LORA_ALPHA,\n    learning_rate=LEARNING_RATE,\n    num_epochs=NUM_EPOCHS,\n    per_device_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRAD_ACCUM,\n    max_seq_length=MAX_SEQ_LEN,\n    bf16=USE_BF16,\n    fp16=not USE_BF16 and torch.cuda.is_available(),\n    dataloader_pin_memory=False,\n)\ntrainer.train()\nprint(f\"Centralized training done in {time.time() - t0:.0f}s\")\n\ncentral_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(device)\ncentral_model = PeftModel.from_pretrained(central_model, centralized_dir)\ncentralized_acc, centralized_report = evaluate_accuracy(central_model, test_ds, \"Centralized\")\ndel central_model; gc.collect(); torch.cuda.empty_cache()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results = {\n    \"baseline_acc\": baseline_acc,\n    \"baseline_report\": baseline_report,\n    \"centralized_acc\": centralized_acc,\n    \"centralized_report\": centralized_report,\n    \"dataset_size\": DATASET_SIZE,\n    \"test_size\": TEST_SIZE,\n    \"num_epochs\": NUM_EPOCHS,\n}\n\noutput_path = f\"{DATA_DIR}/centralized_results.json\"\nwith open(output_path, \"w\") as f:\n    json.dump(results, f, indent=2)\n\nprint(f\"Results saved to {output_path}\")\nprint(f\"\\nBaseline:    {baseline_acc:.1%}\")\nprint(f\"Centralized: {centralized_acc:.1%}\")\nprint(f\"\\nOpen the federated notebook to see the full comparison.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}