{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chorus: Centralized Baseline + Zero-Shot\n",
    "\n",
    "**Run this notebook in parallel with `chorus_centralized_vs_federated.ipynb`.**\n",
    "\n",
    "This notebook trains the centralized (gold standard) model on ALL 6K training examples\n",
    "and evaluates both the zero-shot baseline and centralized model. Results are saved to\n",
    "`/content/centralized_results.json` for the federated notebook to load.\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| Model | `Qwen/Qwen2.5-0.5B` |\n",
    "| Dataset | AG News — 6K train, 2K test |\n",
    "| LoRA | rank=16, alpha=32, 2 epochs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q 'chorus[peft] @ git+https://github.com/varmabudharaju/chorus.git'\n",
    "!pip install -q scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc, os, time, json, logging, random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(name)s] %(message)s', datefmt='%H:%M:%S')\n",
    "\n",
    "# ── Must match federated notebook exactly ─────────────────────────\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "DATASET_SIZE = 6000\n",
    "TEST_SIZE = 2000\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 32\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 8\n",
    "GRAD_ACCUM = 2\n",
    "MAX_SEQ_LEN = 128\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    gpu = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {gpu.name} | VRAM: {gpu.total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "LABEL_NAMES = [\"world\", \"sports\", \"business\", \"scitech\"]\n",
    "LABEL_MAP = {0: \"world\", 1: \"sports\", 2: \"business\", 3: \"scitech\"}\n",
    "\n",
    "raw_train = load_dataset(\"ag_news\", split=\"train\").shuffle(seed=SEED)\n",
    "raw_test = load_dataset(\"ag_news\", split=\"test\").shuffle(seed=SEED)\n",
    "\n",
    "train_ds = raw_train.select(range(DATASET_SIZE))\n",
    "test_ds = raw_test.select(range(TEST_SIZE))\n",
    "\n",
    "print(f\"Train: {len(train_ds)} | Test: {len(test_ds)}\")\n",
    "print(f\"Train labels: {dict(sorted(Counter(train_ds['label']).items()))}\")\n",
    "print(f\"Test labels:  {dict(sorted(Counter(test_ds['label']).items()))}\")\n",
    "\n",
    "def format_example(ex):\n",
    "    return f\"Sentence: {ex['text']}\\nCategory: {LABEL_MAP[ex['label']]}\"\n",
    "\n",
    "central_texts = [format_example(train_ds[j]) for j in range(len(train_ds))]\n",
    "central_ds = Dataset.from_dict({\"text\": central_texts})\n",
    "central_path = \"/content/centralized_train.json\"\n",
    "central_ds.to_json(central_path)\n",
    "print(f\"Saved {len(train_ds)} training examples to {central_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model + Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "USE_BF16 = torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False\n",
    "DTYPE = torch.bfloat16 if USE_BF16 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "label_token_ids = {\n",
    "    0: tokenizer.encode(\" world\", add_special_tokens=False)[0],\n",
    "    1: tokenizer.encode(\" sports\", add_special_tokens=False)[0],\n",
    "    2: tokenizer.encode(\" business\", add_special_tokens=False)[0],\n",
    "    3: tokenizer.encode(\" scitech\", add_special_tokens=False)[0],\n",
    "}\n",
    "print(f\"Label tokens: {label_token_ids}\")\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, test_data, label):\n",
    "    model.eval()\n",
    "    preds, golds = [], []\n",
    "    for ex in test_data:\n",
    "        prompt = f\"Sentence: {ex['text']}\\nCategory:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LEN).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits[:, -1, :]\n",
    "        scores = {lbl: logits[0, tid].item() for lbl, tid in label_token_ids.items()}\n",
    "        preds.append(max(scores, key=scores.get))\n",
    "        golds.append(ex[\"label\"])\n",
    "    acc = accuracy_score(golds, preds)\n",
    "    report = classification_report(\n",
    "        golds, preds, target_names=LABEL_NAMES,\n",
    "        output_dict=True, zero_division=0\n",
    "    )\n",
    "    print(\n",
    "        f\"  [{label}] Accuracy: {acc:.1%}  |  F1: \"\n",
    "        f\"world={report['world']['f1-score']:.2f} \"\n",
    "        f\"sports={report['sports']['f1-score']:.2f} \"\n",
    "        f\"business={report['business']['f1-score']:.2f} \"\n",
    "        f\"scitech={report['scitech']['f1-score']:.2f}\"\n",
    "    )\n",
    "    return acc, report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Baseline (Zero-Shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(device)\n",
    "baseline_acc, baseline_report = evaluate_accuracy(base_model, test_ds, \"Baseline (zero-shot)\")\n",
    "del base_model; gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Centralized Training (Gold Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chorus.client.trainer import LoRATrainer\n",
    "from peft import PeftModel\n",
    "\n",
    "print(f\"Training centralized model on all {DATASET_SIZE} examples, {NUM_EPOCHS} epochs...\")\n",
    "t0 = time.time()\n",
    "\n",
    "centralized_dir = \"/content/adapter_centralized\"\n",
    "trainer = LoRATrainer(\n",
    "    base_model=MODEL_NAME,\n",
    "    dataset=central_path,\n",
    "    output_dir=centralized_dir,\n",
    "    lora_rank=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    per_device_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    max_seq_length=MAX_SEQ_LEN,\n",
    "    bf16=USE_BF16,\n",
    "    fp16=not USE_BF16 and torch.cuda.is_available(),\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "trainer.train()\n",
    "print(f\"Centralized training done in {time.time() - t0:.0f}s\")\n",
    "\n",
    "central_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(device)\n",
    "central_model = PeftModel.from_pretrained(central_model, centralized_dir)\n",
    "centralized_acc, centralized_report = evaluate_accuracy(central_model, test_ds, \"Centralized\")\n",
    "del central_model; gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"baseline_acc\": baseline_acc,\n",
    "    \"baseline_report\": baseline_report,\n",
    "    \"centralized_acc\": centralized_acc,\n",
    "    \"centralized_report\": centralized_report,\n",
    "    \"dataset_size\": DATASET_SIZE,\n",
    "    \"test_size\": TEST_SIZE,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "}\n",
    "\n",
    "output_path = \"/content/centralized_results.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {output_path}\")\n",
    "print(f\"\\nBaseline:    {baseline_acc:.1%}\")\n",
    "print(f\"Centralized: {centralized_acc:.1%}\")\n",
    "print(f\"\\nOpen the federated notebook to see the full comparison.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}