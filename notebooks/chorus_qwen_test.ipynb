{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Chorus: Federated LoRA — IID Validation\n\n**The core question:** 3 organizations each have private data they can't share.\nCan they get a model as good as if they pooled all their data together?\n\n| Model | Training Data | Shares data? |\n|-------|--------------|--------------|\n| **Baseline** (zero-shot) | None | N/A |\n| **Centralized** (gold standard) | All 4500 examples | Yes (unrealistic) |\n| **Individual clients** | ~1500 examples each | N/A (trains alone) |\n| **Federated (Chorus)** | ~1500 each, aggregated | No (privacy preserved) |\n\n**IID split:** Each client gets a random, balanced slice — all classes equally represented.\nThis is the \"fair\" case where federation should clearly help.\n\n**Expected outcome if Chorus works:**\n- Individual clients: decent but limited (only saw 1/3 of data)\n- Federated: close to centralized (combined knowledge without sharing data)\n- Centralized: best possible (saw everything)\n\n| Setting | Value |\n|---------|-------|\n| Model | `Qwen/Qwen2.5-0.5B` (490M, BASE) |\n| Dataset | Financial sentiment — ~4.5K train, 500 test, IID split |\n| LoRA | rank=16, alpha=32, 3 epochs per round |\n| Rounds | 3 |\n| Aggregation | FedEx-LoRA (SVD-optimal) |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q 'chorus[peft] @ git+https://github.com/varmabudharaju/chorus.git'\n!pip install -q scikit-learn\n!pip install -q --upgrade typing_extensions pydantic pydantic-core"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch, gc, os, time, math, json, logging, random\nimport numpy as np\nfrom collections import Counter\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s [%(name)s] %(message)s', datefmt='%H:%M:%S')\n\n# ── Data directory: auto-detect Colab vs RunPod/SageMaker ───────────────\nif os.path.exists(\"/content\"):\n    DATA_DIR = \"/content/chorus_iid\"\nelse:\n    DATA_DIR = os.path.expanduser(\"~/chorus_iid\")\nos.makedirs(DATA_DIR, exist_ok=True)\nprint(f\"Data directory: {DATA_DIR}\")\n\n# ── Hyperparameters ──────────────────────────────────────────────────────\nMODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\nTEST_SIZE = 500\nNUM_CLIENTS = 3\nNUM_ROUNDS = 3\nLORA_RANK = 16\nLORA_ALPHA = 32\nLEARNING_RATE = 3e-4\nNUM_EPOCHS = 3\nBATCH_SIZE = 8\nGRAD_ACCUM = 2\nMAX_SEQ_LEN = 128\nSEED = 42\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\nif device == \"cuda\":\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name} | VRAM: {gpu.total_memory / 1e9:.1f} GB\")\nelse:\n    print(\"WARNING: No GPU! Runtime -> Change runtime type -> T4 GPU\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Dataset — Split Across 3 Clients + Centralized\n\n- 500 held out for testing (stratified)\n- Remaining ~4500 split equally across 3 clients (~1500 each) — **IID** (random, balanced)\n- Centralized model trains on all ~4500 (the \"what if we could share\" gold standard)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset, Dataset, ClassLabel\n\nfull_ds = load_dataset(\"nickmuchi/financial-classification\", split=\"train\")\nfull_ds = full_ds.shuffle(seed=SEED)\nprint(f\"Total dataset: {len(full_ds)} examples\")\n\nLABEL_MAP = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\nprint(f\"Labels: { {LABEL_MAP[k]: v for k, v in sorted(Counter(full_ds['labels']).items())} }\")\n\nfull_ds = full_ds.cast_column(\"labels\", ClassLabel(names=[\"negative\", \"neutral\", \"positive\"]))\nsplit = full_ds.train_test_split(test_size=TEST_SIZE, seed=SEED, stratify_by_column=\"labels\")\ntrain_ds = split[\"train\"]\ntest_ds = split[\"test\"]\nprint(f\"Train: {len(train_ds)} | Test: {len(test_ds)}\")\n\n# IID split — random equal partitions (each client sees all classes equally)\ntrain_ds = train_ds.shuffle(seed=SEED)\nclient_shards = [train_ds.shard(num_shards=NUM_CLIENTS, index=i) for i in range(NUM_CLIENTS)]\n\ndef format_example(ex):\n    return f\"Sentence: {ex['text']}\\nSentiment: {LABEL_MAP[ex['labels']]}\"\n\n# Save client shards\nclient_json_paths = []\nfor i, shard in enumerate(client_shards):\n    texts = [format_example(shard[j]) for j in range(len(shard))]\n    cds = Dataset.from_dict({\"text\": texts})\n    path = f\"{DATA_DIR}/client_{i}.json\"\n    cds.to_json(path)\n    client_json_paths.append(path)\n    dist = {LABEL_MAP[k]: v for k, v in sorted(Counter(shard['labels']).items())}\n    print(f\"Client {i}: {len(shard)} examples | {dist}\")\n\n# Save centralized training set\ncentral_texts = [format_example(train_ds[j]) for j in range(len(train_ds))]\ncentral_ds = Dataset.from_dict({\"text\": central_texts})\ncentral_path = f\"{DATA_DIR}/centralized_train.json\"\ncentral_ds.to_json(central_path)\nprint(f\"\\nCentralized: {len(train_ds)} examples → {central_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model + Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sklearn.metrics import accuracy_score, classification_report\n\nUSE_BF16 = torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False\nDTYPE = torch.bfloat16 if USE_BF16 else torch.float16\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\npos_id = tokenizer.encode(\" positive\", add_special_tokens=False)[0]\nneg_id = tokenizer.encode(\" negative\", add_special_tokens=False)[0]\nneu_id = tokenizer.encode(\" neutral\", add_special_tokens=False)[0]\nlabel_token_ids = {0: neg_id, 1: neu_id, 2: pos_id}\nprint(f\"Model: {MODEL_NAME} | dtype: {DTYPE}\")\n\n\ndef evaluate_accuracy(model, test_data, label):\n    model.eval()\n    preds, golds = [], []\n    for ex in test_data:\n        prompt = f\"Sentence: {ex['text']}\\nSentiment:\"\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LEN).to(device)\n        with torch.no_grad():\n            logits = model(**inputs).logits[:, -1, :]\n        scores = {lbl: logits[0, tid].item() for lbl, tid in label_token_ids.items()}\n        preds.append(max(scores, key=scores.get))\n        golds.append(ex[\"labels\"])\n    acc = accuracy_score(golds, preds)\n    report = classification_report(golds, preds, target_names=[\"negative\", \"neutral\", \"positive\"], output_dict=True, zero_division=0)\n    print(f\"  [{label}] Accuracy: {acc:.1%}  |  F1: neg={report['negative']['f1-score']:.2f} neu={report['neutral']['f1-score']:.2f} pos={report['positive']['f1-score']:.2f}\")\n    return acc, report"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(device)\nbaseline_acc, baseline_report = evaluate_accuracy(base_model, test_ds, \"Baseline (zero-shot)\")\ndel base_model; gc.collect(); torch.cuda.empty_cache()"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 3b: Centralized Training (Gold Standard)\n\nTrain on ALL ~4500 examples — the \"what if everyone could share data\" upper bound.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from chorus.client.trainer import LoRATrainer as CentralTrainer\nfrom peft import PeftModel\n\nprint(f\"Training centralized model on all {len(train_ds)} examples, {NUM_EPOCHS} epochs...\")\nt0 = time.time()\n\ncentralized_dir = f\"{DATA_DIR}/adapter_centralized\"\ncentral_trainer = CentralTrainer(\n    base_model=MODEL_NAME,\n    dataset=central_path,\n    output_dir=centralized_dir,\n    lora_rank=LORA_RANK,\n    lora_alpha=LORA_ALPHA,\n    learning_rate=LEARNING_RATE,\n    num_epochs=NUM_EPOCHS,\n    per_device_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRAD_ACCUM,\n    max_seq_length=MAX_SEQ_LEN,\n    bf16=USE_BF16,\n    fp16=not USE_BF16 and torch.cuda.is_available(),\n    dataloader_pin_memory=False,\n)\ncentral_trainer.train()\nprint(f\"Centralized training done in {time.time() - t0:.0f}s\")\n\ncentral_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(device)\ncentral_model = PeftModel.from_pretrained(central_model, centralized_dir)\ncentralized_acc, centralized_report = evaluate_accuracy(central_model, test_ds, \"Centralized\")\ndel central_model; gc.collect(); torch.cuda.empty_cache()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Start Chorus Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import requests, threading, socket\n\ndef find_free_port():\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind((\"127.0.0.1\", 0))\n        return s.getsockname()[1]\n\nSERVER_PORT = find_free_port()\nSERVER_URL = f\"http://127.0.0.1:{SERVER_PORT}\"\n\nfrom chorus.server.app import configure, app\nimport uvicorn\n\nconfigure(model_id=MODEL_NAME, data_dir=f\"{DATA_DIR}/chorus_data\", strategy=\"fedex-lora\", min_deltas=NUM_CLIENTS)\n\nserver_error = []\ndef run_server():\n    try: uvicorn.run(app, host=\"127.0.0.1\", port=SERVER_PORT, log_level=\"warning\")\n    except Exception as e: server_error.append(str(e))\n\nthreading.Thread(target=run_server, daemon=True).start()\n\nfor i in range(15):\n    time.sleep(1)\n    if server_error: print(f\"ERROR: {server_error[0]}\"); break\n    try:\n        r = requests.get(f\"{SERVER_URL}/health\")\n        if r.status_code == 200: print(f\"Server running at {SERVER_URL} | strategy: {r.json()['strategy']}\"); break\n    except: pass"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Federated Training — 3 Rounds\n",
    "\n",
    "Each round:\n",
    "1. All 3 clients train LoRA on their data (starting from the aggregated adapter after round 1)\n",
    "2. Submit deltas → server aggregates with FedEx-LoRA\n",
    "3. Pull the aggregated adapter → use as starting point for next round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from chorus.client.trainer import LoRATrainer\nfrom chorus.client.sdk import ChorusClient\nfrom safetensors.torch import load_file\nfrom peft import PeftModel, LoraConfig, get_peft_model\n\n# PEFT adapter config — needed so PeftModel.from_pretrained() can load aggregated adapters\nADAPTER_CONFIG = {\n    \"peft_type\": \"LORA\",\n    \"auto_mapping\": None,\n    \"base_model_name_or_path\": MODEL_NAME,\n    \"bias\": \"none\",\n    \"fan_in_fan_out\": False,\n    \"inference_mode\": True,\n    \"init_lora_weights\": True,\n    \"lora_alpha\": LORA_ALPHA,\n    \"lora_dropout\": 0.0,\n    \"r\": LORA_RANK,\n    \"target_modules\": [\"q_proj\", \"v_proj\"],\n    \"task_type\": \"CAUSAL_LM\",\n}\n\n# Track results per round\nround_results = []\nadapter_path_for_next_round = None  # None = train from scratch\n\nfor rnd in range(NUM_ROUNDS):\n    print(f\"\\n{'#'*70}\")\n    print(f\"  ROUND {rnd + 1} / {NUM_ROUNDS}\")\n    print(f\"{'#'*70}\")\n\n    round_client_accs = []\n    round_client_reports = []\n\n    for i in range(NUM_CLIENTS):\n        t0 = time.time()\n        print(f\"\\n  --- Client {i} (round {rnd+1}) ---\")\n\n        output_dir = f\"{DATA_DIR}/adapter_r{rnd}_client_{i}\"\n\n        trainer = LoRATrainer(\n            base_model=MODEL_NAME,\n            dataset=client_json_paths[i],\n            output_dir=output_dir,\n            lora_rank=LORA_RANK,\n            lora_alpha=LORA_ALPHA,\n            learning_rate=LEARNING_RATE,\n            num_epochs=NUM_EPOCHS,\n            per_device_batch_size=BATCH_SIZE,\n            gradient_accumulation_steps=GRAD_ACCUM,\n            max_seq_length=MAX_SEQ_LEN,\n            bf16=USE_BF16,\n            fp16=not USE_BF16 and torch.cuda.is_available(),\n            adapter_path=adapter_path_for_next_round,\n            dataloader_pin_memory=False,\n        )\n\n        adapter_path = trainer.train()\n        elapsed = time.time() - t0\n        print(f\"  Trained in {elapsed:.0f}s\")\n\n        # Evaluate\n        single_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(device)\n        single_model = PeftModel.from_pretrained(single_model, output_dir)\n        acc_i, report_i = evaluate_accuracy(single_model, test_ds, f\"R{rnd+1} Client {i}\")\n        round_client_accs.append(acc_i)\n        round_client_reports.append(report_i)\n        del single_model; gc.collect(); torch.cuda.empty_cache()\n\n        # Submit\n        client = ChorusClient(server=SERVER_URL, model_id=MODEL_NAME, client_id=f\"client-{i}\")\n        result = client.submit_delta(adapter_path=output_dir, dataset_size=len(client_shards[i]))\n        client.close()\n\n        print(f\"  Submitted: {result['deltas_received']}/{result['min_deltas']}\")\n        if result['aggregated']:\n            print(f\"  >>> AGGREGATION TRIGGERED <<<\")\n\n    # Pull aggregated adapter\n    pull_dir = f\"{DATA_DIR}/adapter_federated_r{rnd}\"\n    client = ChorusClient(server=SERVER_URL, model_id=MODEL_NAME)\n    agg_path = client.pull_latest(output_path=pull_dir, adapter_config=ADAPTER_CONFIG)\n    client.close()\n\n    # Evaluate federated model\n    agg_tensors = load_file(str(agg_path))\n    ranks = set(v.shape[0] for k, v in agg_tensors.items() if \"lora_A\" in k)\n    agg_rank = max(ranks)\n\n    fed_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(device)\n    lora_config = LoraConfig(r=agg_rank, lora_alpha=LORA_ALPHA, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.0, bias=\"none\", task_type=\"CAUSAL_LM\")\n    fed_model = get_peft_model(fed_model, lora_config)\n\n    state_dict = fed_model.state_dict()\n    for k, v in agg_tensors.items():\n        peft_key = \"base_model.model.\" + k.replace(\".lora_A.weight\", \".lora_A.default.weight\").replace(\".lora_B.weight\", \".lora_B.default.weight\")\n        if peft_key in state_dict:\n            state_dict[peft_key] = v.to(state_dict[peft_key].dtype).to(device)\n    fed_model.load_state_dict(state_dict)\n\n    fed_acc, fed_report = evaluate_accuracy(fed_model, test_ds, f\"Federated R{rnd+1}\")\n    del fed_model; gc.collect(); torch.cuda.empty_cache()\n\n    round_results.append({\n        \"round\": rnd + 1,\n        \"client_accs\": round_client_accs,\n        \"client_reports\": round_client_reports,\n        \"fed_acc\": fed_acc,\n        \"fed_report\": fed_report,\n    })\n\n    # Use this round's aggregated adapter as starting point for next round\n    adapter_path_for_next_round = pull_dir\n\n    print(f\"\\n  Round {rnd+1} summary: clients={[f'{a:.1%}' for a in round_client_accs]} | federated={fed_acc:.1%}\")\n\nprint(f\"\\n{'#'*70}\")\nprint(\"ALL ROUNDS COMPLETE\")\nprint(f\"{'#'*70}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 75)\nprint(\"RESULTS: Financial Sentiment (IID) — Baseline vs Centralized vs Federated\")\nprint(\"=\" * 75)\n\nprint(f\"\\n{'Model':<45} {'Accuracy':>10} {'Neg F1':>8} {'Neu F1':>8} {'Pos F1':>8}\")\nprint(\"-\" * 81)\n\ndef pr(label, acc, report):\n    print(f\"{label:<45} {acc:>9.1%} {report['negative']['f1-score']:>8.2f} {report['neutral']['f1-score']:>8.2f} {report['positive']['f1-score']:>8.2f}\")\n\npr(\"Baseline (zero-shot)\", baseline_acc, baseline_report)\npr(f\"Centralized (all {len(train_ds)} examples)\", centralized_acc, centralized_report)\nprint()\n\nfor rr in round_results:\n    rnd = rr['round']\n    avg_c = sum(rr['client_accs']) / len(rr['client_accs'])\n    for i in range(NUM_CLIENTS):\n        pr(f\"  Round {rnd} — Client {i}\", rr['client_accs'][i], rr['client_reports'][i])\n    pr(f\"  Round {rnd} — FEDERATED\", rr['fed_acc'], rr['fed_report'])\n    print(f\"  {'':>45} avg client: {avg_c:.1%}\")\n    print()\n\n# Progression\nprint(\"=\" * 75)\nprint(\"PROGRESSION ACROSS ROUNDS\")\nprint(\"=\" * 75)\n\ncol_w = 12\nheader = f\"{'':>20}\" + \"\".join(f\"{'Round '+str(rr['round']):>{col_w}}\" for rr in round_results)\nprint(f\"\\n{header}\")\nprint(\"-\" * len(header))\n\nprint(f\"{'Avg client':>20}\", end=\"\")\nfor rr in round_results:\n    avg = sum(rr['client_accs']) / len(rr['client_accs'])\n    print(f\"{avg:>{col_w}.1%}\", end=\"\")\nprint()\n\nprint(f\"{'Federated':>20}\", end=\"\")\nfor rr in round_results:\n    print(f\"{rr['fed_acc']:>{col_w}.1%}\", end=\"\")\nprint()\n\nprint(f\"{'Centralized':>20}\", end=\"\")\nfor _ in round_results:\n    print(f\"{centralized_acc:>{col_w}.1%}\", end=\"\")\nprint(\"  ← target\")\n\nprint(f\"{'Baseline':>20}\", end=\"\")\nfor _ in round_results:\n    print(f\"{baseline_acc:>{col_w}.1%}\", end=\"\")\nprint(\"  ← floor\")\n\n# Verdict\nprint(\"\\n\" + \"=\" * 75)\nprint(\"VERDICT\")\nprint(\"=\" * 75)\n\nfinal = round_results[-1]\nfinal_fed = final['fed_acc']\nfinal_avg_client = sum(final['client_accs']) / len(final['client_accs'])\nfinal_best = max(final['client_accs'])\nr1_fed = round_results[0]['fed_acc']\n\ngap_to_central = centralized_acc - baseline_acc\nfed_improvement = final_fed - baseline_acc\nif gap_to_central > 0:\n    recovery = fed_improvement / gap_to_central * 100\nelse:\n    recovery = 0\n\nprint(f\"\\nBaseline (zero-shot):  {baseline_acc:.1%}\")\nprint(f\"Centralized (target):  {centralized_acc:.1%}\")\nprint(f\"Federated Round 1:     {r1_fed:.1%}\")\nprint(f\"Federated Round {NUM_ROUNDS}:     {final_fed:.1%}\")\nprint(f\"Avg client (final):    {final_avg_client:.1%}\")\nprint(f\"Best client (final):   {final_best:.1%}\")\nprint(f\"\\nFederation recovered {recovery:.0f}% of the centralized gain over baseline\")\nprint(f\"Round 1 → Round {NUM_ROUNDS} improvement: {(final_fed - r1_fed)*100:+.1f} pp\")\nprint(f\"Federation vs avg client: {(final_fed - final_avg_client)*100:+.1f} pp\")\n\nif final_fed >= centralized_acc * 0.95:\n    print(\"\\n>>> FEDERATION MATCHED CENTRALIZED (within 5%). IID case works! <<<\")\nelif final_fed > final_best:\n    print(\"\\n>>> FEDERATION BEAT EVERY CLIENT! Collaboration works. <<<\")\nelif final_fed > final_avg_client:\n    print(\"\\n>>> Federation beat average client. Partial success. <<<\")\nelse:\n    print(\"\\n>>> Federation underperformed. <<<\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done! Chorus server will shut down when this session ends.\")"
   ]
  }
 ]
}