{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chorus: Federated LoRA — End-to-End Validation\n",
    "\n",
    "**Goal:** Prove that federating LoRA adapters via Chorus produces a model better than any single client.\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| Model | `facebook/opt-125m` (125M params, BASE — not instruction-tuned) |\n",
    "| Dataset | `financial_phrasebank` — 4.8K financial sentiment sentences |\n",
    "| Task | 3-class sentiment: positive / neutral / negative |\n",
    "| Clients | 3 (non-IID split — each sees different label distributions) |\n",
    "| Rounds | 1 |\n",
    "| LoRA rank | 8 |\n",
    "| Aggregation | FedEx-LoRA (SVD-optimal) |\n",
    "| Metric | **Accuracy** (not just loss) |\n",
    "| Runtime | **~15 min total** on T4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q 'chorus[peft] @ git+https://github.com/varmabudharaju/chorus.git'\n",
    "!pip install -q scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc, os, time, math, json, logging\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(name)s] %(message)s', datefmt='%H:%M:%S')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    gpu = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {gpu.name} | VRAM: {gpu.total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU! Go to Runtime -> Change runtime type -> T4 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from datasets import load_dataset, Dataset\n\n# Load financial sentiment dataset (parquet format — no custom scripts)\nds = load_dataset(\"nickmuchi/financial-classification\", split=\"train\")\nprint(f\"Total examples: {len(ds)}\")\n\n# Label mapping: 0=negative, 1=neutral, 2=positive\nLABEL_MAP = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\nlabel_counts = Counter(ds[\"labels\"])\nprint(f\"Label distribution: { {LABEL_MAP[k]: v for k, v in sorted(label_counts.items())} }\")\nprint(f\"Sample: '{ds[0]['text'][:80]}...' -> {LABEL_MAP[ds[0]['labels']]}\")\n\n# Shuffle with seed\nds = ds.shuffle(seed=42)\n\n# Hold out 20% for testing (stratified)\nsplit = ds.train_test_split(test_size=0.2, seed=42, stratify_by_column=\"labels\")\ntrain_ds = split[\"train\"]\ntest_ds = split[\"test\"]\nprint(f\"\\nTrain: {len(train_ds)} | Test: {len(test_ds)}\")\n\n# Non-IID split: group by label, distribute unevenly\nby_label = {0: [], 1: [], 2: []}  # neg, neu, pos\nfor i, ex in enumerate(train_ds):\n    by_label[ex[\"labels\"]].append(i)\n\n# Client 0: mostly neutral (70%) + some positive (30%)\n# Client 1: mostly positive (60%) + some negative (40%)\n# Client 2: mostly negative (60%) + some neutral (40%)\nimport random\nrandom.seed(42)\n\nneg_idx = by_label[0].copy(); random.shuffle(neg_idx)\nneu_idx = by_label[1].copy(); random.shuffle(neu_idx)\npos_idx = by_label[2].copy(); random.shuffle(pos_idx)\n\nn_neg, n_neu, n_pos = len(neg_idx), len(neu_idx), len(pos_idx)\n\nclient_indices = [\n    neu_idx[:int(n_neu*0.7)] + pos_idx[:int(n_pos*0.3)],\n    pos_idx[int(n_pos*0.3):int(n_pos*0.9)] + neg_idx[:int(n_neg*0.4)],\n    neg_idx[int(n_neg*0.4):] + neu_idx[int(n_neu*0.7):],\n]\n\n# Format as text for causal LM training\ndef format_example(ex):\n    return f\"Sentence: {ex['text']}\\nSentiment: {LABEL_MAP[ex['labels']]}\"\n\nclient_datasets = []\nfor i, indices in enumerate(client_indices):\n    random.shuffle(indices)\n    subset = train_ds.select(indices)\n    texts = [format_example(subset[j]) for j in range(len(subset))]\n    cds = Dataset.from_dict({\"text\": texts})\n    path = f\"/content/client_{i}.json\"\n    cds.to_json(path)\n\n    labels_in_client = Counter(subset[\"labels\"])\n    dist = {LABEL_MAP[k]: v for k, v in sorted(labels_in_client.items())}\n    print(f\"Client {i}: {len(cds)} examples | {dist}\")\n    client_datasets.append(cds)\n\nprint(f\"\\nTest set: {len(test_ds)} examples (shared across all evaluations)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Evaluation Setup — Accuracy on Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "MODEL_NAME = \"facebook/opt-125m\"\n",
    "DTYPE = torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Token IDs for our 3 labels\n",
    "pos_id = tokenizer.encode(\" positive\", add_special_tokens=False)[0]\n",
    "neg_id = tokenizer.encode(\" negative\", add_special_tokens=False)[0]\n",
    "neu_id = tokenizer.encode(\" neutral\", add_special_tokens=False)[0]\n",
    "label_token_ids = {0: neg_id, 1: neu_id, 2: pos_id}\n",
    "print(f\"Label tokens: positive={pos_id}, neutral={neu_id}, negative={neg_id}\")\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, test_data, label):\n",
    "    \"\"\"Measure 3-class sentiment accuracy. Returns (accuracy, per_class_report).\"\"\"\n",
    "    model.eval()\n",
    "    preds, golds = [], []\n",
    "\n",
    "    for ex in test_data:\n",
    "        prompt = f\"Sentence: {ex['sentence']}\\nSentiment:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits[:, -1, :]  # logits for next token\n",
    "\n",
    "        # Pick highest prob among our 3 label tokens\n",
    "        scores = {lbl: logits[0, tid].item() for lbl, tid in label_token_ids.items()}\n",
    "        pred = max(scores, key=scores.get)\n",
    "        preds.append(pred)\n",
    "        golds.append(ex[\"label\"])\n",
    "\n",
    "    acc = accuracy_score(golds, preds)\n",
    "    report = classification_report(\n",
    "        golds, preds, target_names=[\"negative\", \"neutral\", \"positive\"],\n",
    "        output_dict=True, zero_division=0\n",
    "    )\n",
    "    print(f\"  [{label}] Accuracy: {acc:.1%}\")\n",
    "    print(f\"    Per-class F1: neg={report['negative']['f1-score']:.2f}  neu={report['neutral']['f1-score']:.2f}  pos={report['positive']['f1-score']:.2f}\")\n",
    "    return acc, report\n",
    "\n",
    "\n",
    "print(\"Evaluation functions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sklearn.metrics import accuracy_score, classification_report\n\nMODEL_NAME = \"facebook/opt-125m\"\nDTYPE = torch.float16\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Token IDs for our 3 labels\npos_id = tokenizer.encode(\" positive\", add_special_tokens=False)[0]\nneg_id = tokenizer.encode(\" negative\", add_special_tokens=False)[0]\nneu_id = tokenizer.encode(\" neutral\", add_special_tokens=False)[0]\nlabel_token_ids = {0: neg_id, 1: neu_id, 2: pos_id}\nprint(f\"Label tokens: positive={pos_id}, neutral={neu_id}, negative={neg_id}\")\n\n\ndef evaluate_accuracy(model, test_data, label):\n    \"\"\"Measure 3-class sentiment accuracy. Returns (accuracy, per_class_report).\"\"\"\n    model.eval()\n    preds, golds = [], []\n\n    for ex in test_data:\n        prompt = f\"Sentence: {ex['text']}\\nSentiment:\"\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n\n        with torch.no_grad():\n            logits = model(**inputs).logits[:, -1, :]  # logits for next token\n\n        # Pick highest prob among our 3 label tokens\n        scores = {lbl: logits[0, tid].item() for lbl, tid in label_token_ids.items()}\n        pred = max(scores, key=scores.get)\n        preds.append(pred)\n        golds.append(ex[\"labels\"])\n\n    acc = accuracy_score(golds, preds)\n    report = classification_report(\n        golds, preds, target_names=[\"negative\", \"neutral\", \"positive\"],\n        output_dict=True, zero_division=0\n    )\n    print(f\"  [{label}] Accuracy: {acc:.1%}\")\n    print(f\"    Per-class F1: neg={report['negative']['f1-score']:.2f}  neu={report['neutral']['f1-score']:.2f}  pos={report['positive']['f1-score']:.2f}\")\n    return acc, report\n\n\nprint(\"Evaluation functions ready.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(device)\n",
    "\n",
    "baseline_acc, baseline_report = evaluate_accuracy(base_model, test_ds, \"Baseline (zero-shot)\")\n",
    "\n",
    "del base_model; gc.collect(); torch.cuda.empty_cache()\n",
    "print(f\"\\nBaseline: {baseline_acc:.1%} (expected ~33% for random 3-class)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Start the Chorus Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, threading, socket\n",
    "\n",
    "def find_free_port():\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind((\"127.0.0.1\", 0))\n",
    "        return s.getsockname()[1]\n",
    "\n",
    "SERVER_PORT = find_free_port()\n",
    "SERVER_URL = f\"http://127.0.0.1:{SERVER_PORT}\"\n",
    "\n",
    "from chorus.server.app import configure, app\n",
    "import uvicorn\n",
    "\n",
    "configure(\n",
    "    model_id=MODEL_NAME,\n",
    "    data_dir=\"/content/chorus_data\",\n",
    "    strategy=\"fedex-lora\",\n",
    "    min_deltas=3,\n",
    ")\n",
    "\n",
    "server_error = []\n",
    "def run_server():\n",
    "    try:\n",
    "        uvicorn.run(app, host=\"127.0.0.1\", port=SERVER_PORT, log_level=\"warning\")\n",
    "    except Exception as e:\n",
    "        server_error.append(str(e))\n",
    "\n",
    "threading.Thread(target=run_server, daemon=True).start()\n",
    "\n",
    "print(f\"Starting Chorus server on port {SERVER_PORT}...\")\n",
    "for i in range(15):\n",
    "    time.sleep(1)\n",
    "    if server_error:\n",
    "        print(f\"ERROR: {server_error[0]}\")\n",
    "        break\n",
    "    try:\n",
    "        r = requests.get(f\"{SERVER_URL}/health\")\n",
    "        if r.status_code == 200:\n",
    "            print(f\"Server running at {SERVER_URL}\")\n",
    "            print(f\"  Strategy: {r.json()['strategy']}\")\n",
    "            break\n",
    "    except requests.ConnectionError:\n",
    "        pass\n",
    "else:\n",
    "    if not server_error:\n",
    "        print(\"ERROR: Server failed to start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train 3 Clients → Submit → Aggregate\n",
    "\n",
    "Each client trains LoRA on its non-IID partition (~1,300 examples, ~2-4 min each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chorus.client.trainer import LoRATrainer\n",
    "from chorus.client.sdk import ChorusClient\n",
    "from peft import PeftModel\n",
    "\n",
    "NUM_CLIENTS = 3\n",
    "client_accs = []\n",
    "client_reports = []\n",
    "\n",
    "for i in range(NUM_CLIENTS):\n",
    "    t0 = time.time()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLIENT {i}: {len(client_datasets[i])} examples\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    output_dir = f\"/content/adapter_client_{i}\"\n",
    "\n",
    "    trainer = LoRATrainer(\n",
    "        base_model=MODEL_NAME,\n",
    "        dataset=f\"/content/client_{i}.json\",\n",
    "        output_dir=output_dir,\n",
    "        lora_rank=8,\n",
    "        lora_alpha=16,\n",
    "        learning_rate=3e-4,\n",
    "        num_epochs=3,\n",
    "        per_device_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        max_seq_length=128,\n",
    "        bf16=False,\n",
    "        fp16=True,\n",
    "        dataloader_pin_memory=False,\n",
    "    )\n",
    "\n",
    "    adapter_path = trainer.train()\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"  Trained in {elapsed:.0f}s\")\n",
    "\n",
    "    # Evaluate this client alone\n",
    "    single_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(device)\n",
    "    single_model = PeftModel.from_pretrained(single_model, output_dir)\n",
    "    acc_i, report_i = evaluate_accuracy(single_model, test_ds, f\"Client {i}\")\n",
    "    client_accs.append(acc_i)\n",
    "    client_reports.append(report_i)\n",
    "    del single_model; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    # Submit to Chorus\n",
    "    client = ChorusClient(server=SERVER_URL, model_id=MODEL_NAME, client_id=f\"client-{i}\")\n",
    "    result = client.submit_delta(adapter_path=output_dir, dataset_size=len(client_datasets[i]))\n",
    "    client.close()\n",
    "\n",
    "    print(f\"  Submitted: {result['deltas_received']}/{result['min_deltas']}\")\n",
    "    if result['aggregated']:\n",
    "        print(f\"  >>> FedEx-LoRA AGGREGATION TRIGGERED! <<<\")\n",
    "\n",
    "print(f\"\\nAll clients done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Pull Federated Adapter → Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Pull aggregated adapter\n",
    "client = ChorusClient(server=SERVER_URL, model_id=MODEL_NAME)\n",
    "agg_path = client.pull_latest(output_path=\"/content/adapter_federated\")\n",
    "client.close()\n",
    "\n",
    "agg_tensors = load_file(str(agg_path))\n",
    "ranks = set(v.shape[0] for k, v in agg_tensors.items() if \"lora_A\" in k)\n",
    "agg_rank = max(ranks)\n",
    "print(f\"Aggregated adapter: {len(agg_tensors)} tensors, rank {agg_rank}\")\n",
    "print(f\"File size: {os.path.getsize(agg_path) / 1e6:.2f} MB\")\n",
    "\n",
    "# Load into model\n",
    "fed_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(device)\n",
    "lora_config = LoraConfig(\n",
    "    r=agg_rank, lora_alpha=agg_rank,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.0, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "fed_model = get_peft_model(fed_model, lora_config)\n",
    "\n",
    "state_dict = fed_model.state_dict()\n",
    "loaded = 0\n",
    "for k, v in agg_tensors.items():\n",
    "    peft_key = (\n",
    "        \"base_model.model.\" +\n",
    "        k.replace(\".lora_A.weight\", \".lora_A.default.weight\")\n",
    "         .replace(\".lora_B.weight\", \".lora_B.default.weight\")\n",
    "    )\n",
    "    if peft_key in state_dict:\n",
    "        state_dict[peft_key] = v.to(state_dict[peft_key].dtype).to(device)\n",
    "        loaded += 1\n",
    "fed_model.load_state_dict(state_dict)\n",
    "print(f\"Loaded {loaded}/{len(agg_tensors)} tensors\")\n",
    "\n",
    "# Evaluate\n",
    "fed_acc, fed_report = evaluate_accuracy(fed_model, test_ds, \"Federated (FedEx-LoRA)\")\n",
    "del fed_model; gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTS: Financial Sentiment Classification\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Accuracy table\n",
    "print(f\"\\n{'Model':<40} {'Accuracy':>10} {'Neg F1':>8} {'Neu F1':>8} {'Pos F1':>8}\")\n",
    "print(\"-\" * 76)\n",
    "\n",
    "def print_row(label, acc, report):\n",
    "    neg_f1 = report['negative']['f1-score']\n",
    "    neu_f1 = report['neutral']['f1-score']\n",
    "    pos_f1 = report['positive']['f1-score']\n",
    "    print(f\"{label:<40} {acc:>9.1%} {neg_f1:>8.2f} {neu_f1:>8.2f} {pos_f1:>8.2f}\")\n",
    "\n",
    "print_row(\"Baseline (zero-shot, no training)\", baseline_acc, baseline_report)\n",
    "for i in range(NUM_CLIENTS):\n",
    "    print_row(f\"Client {i} alone\", client_accs[i], client_reports[i])\n",
    "print(\"-\" * 76)\n",
    "print_row(\"Federated (FedEx-LoRA, 3 clients)\", fed_acc, fed_report)\n",
    "\n",
    "# Summary\n",
    "avg_client_acc = sum(client_accs) / len(client_accs)\n",
    "best_client_acc = max(client_accs)\n",
    "best_client_idx = client_accs.index(best_client_acc)\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 76)\n",
    "print(f\"\\nBaseline accuracy:       {baseline_acc:.1%}\")\n",
    "print(f\"Best single client:      {best_client_acc:.1%} (Client {best_client_idx})\")\n",
    "print(f\"Average client:          {avg_client_acc:.1%}\")\n",
    "print(f\"Federated:               {fed_acc:.1%}\")\n",
    "print(f\"\\nFederation vs baseline:     +{(fed_acc - baseline_acc)*100:.1f} percentage points\")\n",
    "print(f\"Federation vs best client:  {(fed_acc - best_client_acc)*100:+.1f} percentage points\")\n",
    "print(f\"Federation vs avg client:   {(fed_acc - avg_client_acc)*100:+.1f} percentage points\")\n",
    "\n",
    "if fed_acc > best_client_acc:\n",
    "    print(f\"\\n>>> FEDERATION BEAT EVERY INDIVIDUAL CLIENT! Chorus works. <<<\")\n",
    "elif fed_acc > avg_client_acc:\n",
    "    print(f\"\\n>>> Federation beat the average client but not the best. Partial success. <<<\")\n",
    "else:\n",
    "    print(f\"\\n>>> Federation did not beat avg client. Needs more rounds or tuning. <<<\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visual — Per-Class Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show where federation helps: per-class F1 comparison\n",
    "classes = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "print(\"\\nPer-Class F1 Scores (where federation shines):\")\n",
    "print(f\"\\n{'':>25} {'Negative':>10} {'Neutral':>10} {'Positive':>10}\")\n",
    "print(\"-\" * 58)\n",
    "print(f\"{'Baseline':<25} {baseline_report['negative']['f1-score']:>10.2f} {baseline_report['neutral']['f1-score']:>10.2f} {baseline_report['positive']['f1-score']:>10.2f}\")\n",
    "for i in range(NUM_CLIENTS):\n",
    "    r = client_reports[i]\n",
    "    print(f\"{'Client '+str(i):<25} {r['negative']['f1-score']:>10.2f} {r['neutral']['f1-score']:>10.2f} {r['positive']['f1-score']:>10.2f}\")\n",
    "print(\"-\" * 58)\n",
    "print(f\"{'Federated':<25} {fed_report['negative']['f1-score']:>10.2f} {fed_report['neutral']['f1-score']:>10.2f} {fed_report['positive']['f1-score']:>10.2f}\")\n",
    "\n",
    "print(\"\\nKey insight: Each client is weak on classes they saw less of.\")\n",
    "print(\"The federated model should be strong across ALL classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done! All results above.\")\n",
    "print(f\"\\nTotal test examples: {len(test_ds)}\")\n",
    "print(f\"Chorus server will shut down when this session ends.\")"
   ]
  }
 ]
}