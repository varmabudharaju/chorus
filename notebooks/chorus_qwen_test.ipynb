{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chorus: Federated LoRA — End-to-End Validation\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| Model | `Qwen/Qwen2.5-0.5B` (490M, BASE) |\n",
    "| Dataset | Financial sentiment — 2K examples |\n",
    "| Splits | **IID** (random equal) across 3 clients |\n",
    "| LoRA rank | **16** |\n",
    "| Rounds | **3** (train → submit → aggregate → pull → retrain) |\n",
    "| Aggregation | FedEx-LoRA |\n",
    "| Metric | Accuracy + per-class F1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q 'chorus[peft] @ git+https://github.com/varmabudharaju/chorus.git'\n",
    "!pip install -q scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc, os, time, math, json, logging, random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(name)s] %(message)s', datefmt='%H:%M:%S')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    gpu = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU: {gpu.name} | VRAM: {gpu.total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU! Runtime -> Change runtime type -> T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset — IID Split Across 3 Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, ClassLabel\n",
    "\n",
    "full_ds = load_dataset(\"nickmuchi/financial-classification\", split=\"train\")\n",
    "full_ds = full_ds.shuffle(seed=42).select(range(2000))\n",
    "print(f\"Using {len(full_ds)} examples\")\n",
    "\n",
    "LABEL_MAP = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "print(f\"Labels: { {LABEL_MAP[k]: v for k, v in sorted(Counter(full_ds['labels']).items())} }\")\n",
    "\n",
    "full_ds = full_ds.cast_column(\"labels\", ClassLabel(names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "split = full_ds.train_test_split(test_size=200, seed=42, stratify_by_column=\"labels\")\n",
    "train_ds = split[\"train\"]\n",
    "test_ds = split[\"test\"]\n",
    "print(f\"Train: {len(train_ds)} | Test: {len(test_ds)}\")\n",
    "\n",
    "# IID split — random equal partitions (each client sees all classes)\n",
    "train_ds = train_ds.shuffle(seed=42)\n",
    "NUM_CLIENTS = 3\n",
    "client_shards = [train_ds.shard(num_shards=NUM_CLIENTS, index=i) for i in range(NUM_CLIENTS)]\n",
    "\n",
    "def format_example(ex):\n",
    "    return f\"Sentence: {ex['text']}\\nSentiment: {LABEL_MAP[ex['labels']]}\"\n",
    "\n",
    "client_json_paths = []\n",
    "for i, shard in enumerate(client_shards):\n",
    "    texts = [format_example(shard[j]) for j in range(len(shard))]\n",
    "    cds = Dataset.from_dict({\"text\": texts})\n",
    "    path = f\"/content/client_{i}.json\"\n",
    "    cds.to_json(path)\n",
    "    client_json_paths.append(path)\n",
    "    dist = {LABEL_MAP[k]: v for k, v in sorted(Counter(shard['labels']).items())}\n",
    "    print(f\"Client {i}: {len(shard)} examples | {dist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model + Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "USE_BF16 = torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False\n",
    "DTYPE = torch.bfloat16 if USE_BF16 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "pos_id = tokenizer.encode(\" positive\", add_special_tokens=False)[0]\n",
    "neg_id = tokenizer.encode(\" negative\", add_special_tokens=False)[0]\n",
    "neu_id = tokenizer.encode(\" neutral\", add_special_tokens=False)[0]\n",
    "label_token_ids = {0: neg_id, 1: neu_id, 2: pos_id}\n",
    "print(f\"Model: {MODEL_NAME} | dtype: {DTYPE}\")\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, test_data, label):\n",
    "    model.eval()\n",
    "    preds, golds = [], []\n",
    "    for ex in test_data:\n",
    "        prompt = f\"Sentence: {ex['text']}\\nSentiment:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits[:, -1, :]\n",
    "        scores = {lbl: logits[0, tid].item() for lbl, tid in label_token_ids.items()}\n",
    "        preds.append(max(scores, key=scores.get))\n",
    "        golds.append(ex[\"labels\"])\n",
    "    acc = accuracy_score(golds, preds)\n",
    "    report = classification_report(golds, preds, target_names=[\"negative\", \"neutral\", \"positive\"], output_dict=True, zero_division=0)\n",
    "    print(f\"  [{label}] Accuracy: {acc:.1%}  |  F1: neg={report['negative']['f1-score']:.2f} neu={report['neutral']['f1-score']:.2f} pos={report['positive']['f1-score']:.2f}\")\n",
    "    return acc, report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(device)\n",
    "baseline_acc, baseline_report = evaluate_accuracy(base_model, test_ds, \"Baseline (zero-shot)\")\n",
    "del base_model; gc.collect(); torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Start Chorus Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, threading, socket\n",
    "\n",
    "def find_free_port():\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind((\"127.0.0.1\", 0))\n",
    "        return s.getsockname()[1]\n",
    "\n",
    "SERVER_PORT = find_free_port()\n",
    "SERVER_URL = f\"http://127.0.0.1:{SERVER_PORT}\"\n",
    "\n",
    "from chorus.server.app import configure, app\n",
    "import uvicorn\n",
    "\n",
    "configure(model_id=MODEL_NAME, data_dir=\"/content/chorus_data\", strategy=\"fedex-lora\", min_deltas=3)\n",
    "\n",
    "server_error = []\n",
    "def run_server():\n",
    "    try: uvicorn.run(app, host=\"127.0.0.1\", port=SERVER_PORT, log_level=\"warning\")\n",
    "    except Exception as e: server_error.append(str(e))\n",
    "\n",
    "threading.Thread(target=run_server, daemon=True).start()\n",
    "\n",
    "for i in range(15):\n",
    "    time.sleep(1)\n",
    "    if server_error: print(f\"ERROR: {server_error[0]}\"); break\n",
    "    try:\n",
    "        r = requests.get(f\"{SERVER_URL}/health\")\n",
    "        if r.status_code == 200: print(f\"Server running at {SERVER_URL} | strategy: {r.json()['strategy']}\"); break\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Federated Training — 3 Rounds\n",
    "\n",
    "Each round:\n",
    "1. All 3 clients train LoRA on their data (starting from the aggregated adapter after round 1)\n",
    "2. Submit deltas → server aggregates with FedEx-LoRA\n",
    "3. Pull the aggregated adapter → use as starting point for next round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from chorus.client.trainer import LoRATrainer\nfrom chorus.client.sdk import ChorusClient\nfrom safetensors.torch import load_file\nfrom peft import PeftModel, LoraConfig, get_peft_model\n\nNUM_ROUNDS = 3\nLORA_RANK = 16\nLORA_ALPHA = LORA_RANK * 2  # 32 — must match training config everywhere\n\n# PEFT adapter config — needed so PeftModel.from_pretrained() can load aggregated adapters\nADAPTER_CONFIG = {\n    \"peft_type\": \"LORA\",\n    \"auto_mapping\": None,\n    \"base_model_name_or_path\": MODEL_NAME,\n    \"bias\": \"none\",\n    \"fan_in_fan_out\": False,\n    \"inference_mode\": True,\n    \"init_lora_weights\": True,\n    \"lora_alpha\": LORA_ALPHA,\n    \"lora_dropout\": 0.0,\n    \"r\": LORA_RANK,\n    \"target_modules\": [\"q_proj\", \"v_proj\"],\n    \"task_type\": \"CAUSAL_LM\",\n}\n\n# Track results per round\nround_results = []  # list of {round, client_accs, fed_acc, ...}\nadapter_path_for_next_round = None  # None = train from scratch\n\nfor rnd in range(NUM_ROUNDS):\n    print(f\"\\n{'#'*70}\")\n    print(f\"  ROUND {rnd + 1} / {NUM_ROUNDS}\")\n    print(f\"{'#'*70}\")\n\n    round_client_accs = []\n    round_client_reports = []\n\n    for i in range(NUM_CLIENTS):\n        t0 = time.time()\n        print(f\"\\n  --- Client {i} (round {rnd+1}) ---\")\n\n        output_dir = f\"/content/adapter_r{rnd}_client_{i}\"\n\n        trainer = LoRATrainer(\n            base_model=MODEL_NAME,\n            dataset=client_json_paths[i],\n            output_dir=output_dir,\n            lora_rank=LORA_RANK,\n            lora_alpha=LORA_ALPHA,\n            learning_rate=3e-4,\n            num_epochs=3,\n            per_device_batch_size=8,\n            gradient_accumulation_steps=2,\n            max_seq_length=128,\n            bf16=USE_BF16,\n            fp16=not USE_BF16 and torch.cuda.is_available(),\n            adapter_path=adapter_path_for_next_round,\n            dataloader_pin_memory=False,\n        )\n\n        adapter_path = trainer.train()\n        elapsed = time.time() - t0\n        print(f\"  Trained in {elapsed:.0f}s\")\n\n        # Evaluate\n        single_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(device)\n        single_model = PeftModel.from_pretrained(single_model, output_dir)\n        acc_i, report_i = evaluate_accuracy(single_model, test_ds, f\"R{rnd+1} Client {i}\")\n        round_client_accs.append(acc_i)\n        round_client_reports.append(report_i)\n        del single_model; gc.collect(); torch.cuda.empty_cache()\n\n        # Submit\n        client = ChorusClient(server=SERVER_URL, model_id=MODEL_NAME, client_id=f\"client-{i}\")\n        result = client.submit_delta(adapter_path=output_dir, dataset_size=len(client_shards[i]))\n        client.close()\n\n        print(f\"  Submitted: {result['deltas_received']}/{result['min_deltas']}\")\n        if result['aggregated']:\n            print(f\"  >>> AGGREGATION TRIGGERED <<<\")\n\n    # Pull aggregated adapter (with adapter_config.json so next round can load it)\n    pull_dir = f\"/content/adapter_federated_r{rnd}\"\n    client = ChorusClient(server=SERVER_URL, model_id=MODEL_NAME)\n    agg_path = client.pull_latest(output_path=pull_dir, adapter_config=ADAPTER_CONFIG)\n    client.close()\n\n    # Evaluate federated model — use SAME lora_alpha as training!\n    agg_tensors = load_file(str(agg_path))\n    ranks = set(v.shape[0] for k, v in agg_tensors.items() if \"lora_A\" in k)\n    agg_rank = max(ranks)\n\n    fed_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(device)\n    lora_config = LoraConfig(r=agg_rank, lora_alpha=LORA_ALPHA, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.0, bias=\"none\", task_type=\"CAUSAL_LM\")\n    fed_model = get_peft_model(fed_model, lora_config)\n\n    state_dict = fed_model.state_dict()\n    for k, v in agg_tensors.items():\n        peft_key = \"base_model.model.\" + k.replace(\".lora_A.weight\", \".lora_A.default.weight\").replace(\".lora_B.weight\", \".lora_B.default.weight\")\n        if peft_key in state_dict:\n            state_dict[peft_key] = v.to(state_dict[peft_key].dtype).to(device)\n    fed_model.load_state_dict(state_dict)\n\n    fed_acc, fed_report = evaluate_accuracy(fed_model, test_ds, f\"Federated R{rnd+1}\")\n    del fed_model; gc.collect(); torch.cuda.empty_cache()\n\n    round_results.append({\n        \"round\": rnd + 1,\n        \"client_accs\": round_client_accs,\n        \"client_reports\": round_client_reports,\n        \"fed_acc\": fed_acc,\n        \"fed_report\": fed_report,\n    })\n\n    # Use this round's aggregated adapter as starting point for next round\n    adapter_path_for_next_round = pull_dir\n\n    print(f\"\\n  Round {rnd+1} summary: clients={[f'{a:.1%}' for a in round_client_accs]} | federated={fed_acc:.1%}\")\n\nprint(f\"\\n{'#'*70}\")\nprint(\"ALL ROUNDS COMPLETE\")\nprint(f\"{'#'*70}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 75)\n",
    "print(\"RESULTS: Financial Sentiment Classification — 3 Federated Rounds\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "print(f\"\\n{'Model':<45} {'Accuracy':>10} {'Neg F1':>8} {'Neu F1':>8} {'Pos F1':>8}\")\n",
    "print(\"-\" * 81)\n",
    "\n",
    "def pr(label, acc, report):\n",
    "    print(f\"{label:<45} {acc:>9.1%} {report['negative']['f1-score']:>8.2f} {report['neutral']['f1-score']:>8.2f} {report['positive']['f1-score']:>8.2f}\")\n",
    "\n",
    "pr(\"Baseline (zero-shot)\", baseline_acc, baseline_report)\n",
    "print()\n",
    "\n",
    "for rr in round_results:\n",
    "    rnd = rr['round']\n",
    "    avg_c = sum(rr['client_accs']) / len(rr['client_accs'])\n",
    "    for i in range(NUM_CLIENTS):\n",
    "        pr(f\"  Round {rnd} — Client {i}\", rr['client_accs'][i], rr['client_reports'][i])\n",
    "    pr(f\"  Round {rnd} — FEDERATED\", rr['fed_acc'], rr['fed_report'])\n",
    "    print(f\"  {'':>45} avg client: {avg_c:.1%}\")\n",
    "    print()\n",
    "\n",
    "# Progression\n",
    "print(\"=\" * 75)\n",
    "print(\"PROGRESSION ACROSS ROUNDS\")\n",
    "print(\"=\" * 75)\n",
    "print(f\"\\n{'':>20} {'Round 1':>10} {'Round 2':>10} {'Round 3':>10}\")\n",
    "print(\"-\" * 52)\n",
    "print(f\"{'Avg client':>20}\", end=\"\")\n",
    "for rr in round_results:\n",
    "    avg = sum(rr['client_accs']) / len(rr['client_accs'])\n",
    "    print(f\" {avg:>9.1%}\", end=\"\")\n",
    "print()\n",
    "print(f\"{'Federated':>20}\", end=\"\")\n",
    "for rr in round_results:\n",
    "    print(f\" {rr['fed_acc']:>9.1%}\", end=\"\")\n",
    "print()\n",
    "\n",
    "final = round_results[-1]\n",
    "final_avg_client = sum(final['client_accs']) / len(final['client_accs'])\n",
    "final_best = max(final['client_accs'])\n",
    "\n",
    "print(f\"\\n\\nBaseline:           {baseline_acc:.1%}\")\n",
    "print(f\"Final federated:    {final['fed_acc']:.1%}\")\n",
    "print(f\"Final avg client:   {final_avg_client:.1%}\")\n",
    "print(f\"Final best client:  {final_best:.1%}\")\n",
    "\n",
    "r1_fed = round_results[0]['fed_acc']\n",
    "r3_fed = round_results[-1]['fed_acc']\n",
    "print(f\"\\nRound 1 → Round 3 federated improvement: {(r3_fed - r1_fed)*100:+.1f} pp\")\n",
    "\n",
    "if r3_fed > final_best:\n",
    "    print(\"\\n>>> FEDERATION BEAT EVERY CLIENT! Chorus works. <<<\")\n",
    "elif r3_fed > final_avg_client:\n",
    "    print(\"\\n>>> Federation beat average client. Partial success. <<<\")\n",
    "else:\n",
    "    print(\"\\n>>> Federation underperformed. <<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done! Chorus server will shut down when this session ends.\")"
   ]
  }
 ]
}