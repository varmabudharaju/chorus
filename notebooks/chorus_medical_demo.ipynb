{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chorus: Federated LoRA on Medical Data\n",
    "\n",
    "This notebook demonstrates **federated fine-tuning** of TinyLlama on medical Q&A data using the actual Chorus framework — server, clients, and all.\n",
    "\n",
    "**Scenario:** Two hospitals each have private medical flashcard data. Neither can share their data. Instead, each trains a LoRA adapter locally and submits it to a Chorus server, which aggregates them using **FedExLoRA** (exact aggregation with heterogeneous rank support).\n",
    "\n",
    "We compare:\n",
    "1. **Base model** (no fine-tuning) — how TinyLlama answers medical questions out of the box\n",
    "2. **Single hospital** (trained on only half the data)\n",
    "3. **Federated model** (both hospitals' adapters merged by Chorus server)\n",
    "\n",
    "**Runtime:** ~15 min on Colab T4 GPU (free tier)\n",
    "\n",
    "**Architecture being tested:**\n",
    "```\n",
    "Hospital A (rank 8)  ──submit──►  Chorus Server  ◄──submit──  Hospital B (rank 16)\n",
    "                                       │\n",
    "                                  FedExLoRA aggregation\n",
    "                                  (exact, heterogeneous)\n",
    "                                       │\n",
    "                              Aggregated adapter (rank 16)\n",
    "                                       │\n",
    "                                  ◄──pull──\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install Chorus + Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Chorus and install with PEFT training support\n",
    "!git clone https://github.com/varmabudharaju/chorus.git /content/chorus 2>/dev/null || echo 'Already cloned'\n",
    "!pip install -q -e '/content/chorus[peft]'\n",
    "!pip install -q bitsandbytes  # For efficient GPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import subprocess\n",
    "import signal\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be very slow.\")\n",
    "    print(\"Go to Runtime → Change runtime type → T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Medical Datasets for Two Hospitals\n",
    "\n",
    "We use [medalpaca/medical_meadow_medical_flashcards](https://huggingface.co/datasets/medalpaca/medical_meadow_medical_flashcards) — 33K medical Q&A pairs from clinical flashcards.\n",
    "\n",
    "We split it in half to simulate two hospitals with **different patient populations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\", split=\"train\")\n",
    "print(f\"Total medical flashcards: {len(ds)}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(f\"  Instruction: {ds[0]['instruction']}\")\n",
    "print(f\"  Question:    {ds[0]['input'][:120]}\")\n",
    "print(f\"  Answer:      {ds[0]['output'][:120]}\")\n",
    "\n",
    "# Shuffle and split between two hospitals\n",
    "ds = ds.shuffle(seed=42)\n",
    "split = len(ds) // 2\n",
    "hospital_a_data = ds.select(range(split))\n",
    "hospital_b_data = ds.select(range(split, len(ds)))\n",
    "\n",
    "# Save as JSON for Chorus LoRATrainer (it accepts HF dataset names or local files)\n",
    "hospital_a_data.to_json(\"/content/hospital_a.json\")\n",
    "hospital_b_data.to_json(\"/content/hospital_b.json\")\n",
    "\n",
    "print(f\"\\nHospital A: {len(hospital_a_data)} examples → /content/hospital_a.json\")\n",
    "print(f\"Hospital B: {len(hospital_b_data)} examples → /content/hospital_b.json\")\n",
    "\n",
    "# Hold out eval examples (5 from each hospital's data)\n",
    "eval_examples = [\n",
    "    {\"q\": hospital_a_data[i][\"input\"], \"a\": hospital_a_data[i][\"output\"]}\n",
    "    for i in range(0, 50, 5)\n",
    "] + [\n",
    "    {\"q\": hospital_b_data[i][\"input\"], \"a\": hospital_b_data[i][\"output\"]}\n",
    "    for i in range(0, 50, 5)\n",
    "]\n",
    "print(f\"Held out {len(eval_examples)} examples for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Setup\n",
    "\n",
    "We measure **medical QA perplexity** (lower = the model is more confident about correct medical answers) and compare generated answers qualitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "MEDICAL_QUESTIONS = [\n",
    "    \"What are the classic symptoms of diabetes mellitus?\",\n",
    "    \"What is the first-line treatment for hypertension?\",\n",
    "    \"What does an elevated troponin level indicate?\",\n",
    "    \"What is the difference between Type 1 and Type 2 diabetes?\",\n",
    "    \"What are the side effects of metformin?\",\n",
    "]\n",
    "\n",
    "\n",
    "def format_prompt(question):\n",
    "    return (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        f\"### Instruction:\\nAnswer this medical question truthfully: {question}\\n\\n\"\n",
    "        \"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_model(model, label, eval_data=None, gen_questions=None):\n",
    "    \"\"\"Evaluate a model: perplexity on medical QA + qualitative generation.\"\"\"\n",
    "    model.eval()\n",
    "    results = {\"label\": label}\n",
    "\n",
    "    if eval_data:\n",
    "        total_loss = 0.0\n",
    "        for ex in eval_data:\n",
    "            text = f\"Question: {ex['q']}\\nAnswer: {ex['a']}\"\n",
    "            inputs = tokenizer(\n",
    "                text, return_tensors=\"pt\", truncation=True, max_length=256\n",
    "            ).to(device)\n",
    "            with torch.no_grad():\n",
    "                loss = model(**inputs, labels=inputs[\"input_ids\"]).loss\n",
    "            total_loss += loss.item()\n",
    "        results[\"perplexity\"] = math.exp(total_loss / len(eval_data))\n",
    "        print(f\"  [{label}] Medical QA perplexity: {results['perplexity']:.2f}\")\n",
    "\n",
    "    if gen_questions:\n",
    "        results[\"generations\"] = []\n",
    "        for q in gen_questions:\n",
    "            prompt = format_prompt(q)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(\n",
    "                    **inputs, max_new_tokens=150, do_sample=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "            answer = tokenizer.decode(\n",
    "                out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True\n",
    "            ).strip()\n",
    "            results[\"generations\"].append({\"q\": q, \"a\": answer})\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def load_adapter_into_model(adapter_tensors, rank):\n",
    "    \"\"\"Load base model + inject aggregated adapter tensors from Chorus.\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME, torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "    config = LoraConfig(\n",
    "        r=rank, lora_alpha=rank, target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.0, bias=\"none\", task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "    loaded = 0\n",
    "    for k, v in adapter_tensors.items():\n",
    "        peft_key = (\n",
    "            \"base_model.model.\" +\n",
    "            k.replace(\".lora_A.weight\", \".lora_A.default.weight\")\n",
    "             .replace(\".lora_B.weight\", \".lora_B.default.weight\")\n",
    "        )\n",
    "        if peft_key in state_dict:\n",
    "            state_dict[peft_key] = v.to(state_dict[peft_key].dtype).to(device)\n",
    "            loaded += 1\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(f\"  Loaded {loaded}/{len(adapter_tensors)} tensors into model\")\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"Evaluation functions ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline: Evaluate Base Model (No Medical Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BASELINE: TinyLlama-Chat (no medical fine-tuning)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "baseline_results = evaluate_model(\n",
    "    base_model, \"Base Model\",\n",
    "    eval_data=eval_examples,\n",
    "    gen_questions=MEDICAL_QUESTIONS,\n",
    ")\n",
    "\n",
    "print(\"\\nSample answers from BASE model:\")\n",
    "for g in baseline_results[\"generations\"][:3]:\n",
    "    print(f\"  Q: {g['q']}\")\n",
    "    print(f\"  A: {g['a'][:200]}\")\n",
    "    print()\n",
    "\n",
    "del base_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start the Chorus Server\n",
    "\n",
    "We start the actual Chorus aggregation server in the background. It will:\n",
    "- Listen for LoRA adapter submissions from clients\n",
    "- Aggregate with **FedExLoRA** once 2 deltas arrive\n",
    "- Support **heterogeneous ranks** (Hospital A = rank 8, Hospital B = rank 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Start Chorus server in background\n",
    "server_proc = subprocess.Popen(\n",
    "    [\n",
    "        \"chorus\", \"server\",\n",
    "        \"--model\", MODEL_NAME,\n",
    "        \"--strategy\", \"fedex-lora\",\n",
    "        \"--min-deltas\", \"2\",\n",
    "        \"-v\",\n",
    "    ],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "# Wait for server to be ready\n",
    "print(\"Starting Chorus server...\")\n",
    "for i in range(30):\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        r = requests.get(\"http://localhost:8080/health\")\n",
    "        if r.status_code == 200:\n",
    "            print(f\"Chorus server is running!\")\n",
    "            print(f\"  Health: {r.json()}\")\n",
    "            break\n",
    "    except requests.ConnectionError:\n",
    "        pass\n",
    "else:\n",
    "    print(\"ERROR: Server failed to start. Check logs:\")\n",
    "    print(server_proc.stdout.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hospital A: Train LoRA Adapter (Rank 8)\n",
    "\n",
    "Using Chorus's `LoRATrainer` — the same component a real client would use.\n",
    "\n",
    "Hospital A has a smaller compute budget → **rank 8** LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chorus.client.trainer import LoRATrainer\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HOSPITAL A: Training LoRA adapter (rank 8)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trainer_a = LoRATrainer(\n",
    "    base_model=MODEL_NAME,\n",
    "    dataset=\"/content/hospital_a.json\",\n",
    "    output_dir=\"/content/adapter_hospital_a\",\n",
    "    lora_rank=8,\n",
    "    lora_alpha=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_epochs=1,\n",
    "    per_device_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_seq_length=256,\n",
    "    bf16=torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False,\n",
    "    fp16=not (torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False),\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "adapter_path_a = trainer_a.train()\n",
    "print(f\"\\nHospital A adapter saved to: {adapter_path_a}\")\n",
    "print(f\"Dataset size: {trainer_a.get_dataset_size()} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Hospital A's adapter (single hospital, no federation)\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"Evaluating Hospital A's adapter (single hospital, no federation):\")\n",
    "model_a = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16\n",
    ").to(device)\n",
    "model_a = PeftModel.from_pretrained(model_a, \"/content/adapter_hospital_a\")\n",
    "model_a.eval()\n",
    "\n",
    "single_a_results = evaluate_model(\n",
    "    model_a, \"Hospital A only (rank 8)\",\n",
    "    eval_data=eval_examples,\n",
    "    gen_questions=MEDICAL_QUESTIONS,\n",
    ")\n",
    "\n",
    "del model_a\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hospital A Submits to Chorus Server\n",
    "\n",
    "Using the actual `ChorusClient.submit_delta()` — sends the adapter over HTTP to the running Chorus server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chorus.client.sdk import ChorusClient\n",
    "\n",
    "# Hospital A submits its adapter\n",
    "client_a = ChorusClient(\n",
    "    server=\"http://localhost:8080\",\n",
    "    model_id=MODEL_NAME,\n",
    "    client_id=\"hospital-a\",\n",
    ")\n",
    "\n",
    "result_a = client_a.submit_delta(\n",
    "    adapter_path=\"/content/adapter_hospital_a\",\n",
    "    dataset_size=len(hospital_a_data),\n",
    ")\n",
    "\n",
    "print(f\"Hospital A submitted!\")\n",
    "print(f\"  Round: {result_a['round_id']}\")\n",
    "print(f\"  Deltas received: {result_a['deltas_received']}/{result_a['min_deltas']}\")\n",
    "print(f\"  Aggregated: {result_a['aggregated']}\")\n",
    "print(f\"\\n  (Waiting for Hospital B before aggregation triggers...)\")\n",
    "\n",
    "client_a.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hospital B: Train LoRA Adapter (Rank 16)\n",
    "\n",
    "Hospital B has more compute → **rank 16** LoRA.\n",
    "\n",
    "This tests Chorus's **heterogeneous rank** support — the server must handle rank 8 + rank 16 adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"HOSPITAL B: Training LoRA adapter (rank 16)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trainer_b = LoRATrainer(\n",
    "    base_model=MODEL_NAME,\n",
    "    dataset=\"/content/hospital_b.json\",\n",
    "    output_dir=\"/content/adapter_hospital_b\",\n",
    "    lora_rank=16,\n",
    "    lora_alpha=16,\n",
    "    learning_rate=2e-4,\n",
    "    num_epochs=1,\n",
    "    per_device_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_seq_length=256,\n",
    "    bf16=torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False,\n",
    "    fp16=not (torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False),\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "adapter_path_b = trainer_b.train()\n",
    "print(f\"\\nHospital B adapter saved to: {adapter_path_b}\")\n",
    "print(f\"Dataset size: {trainer_b.get_dataset_size()} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Hospital B's adapter (single hospital, no federation)\n",
    "print(\"Evaluating Hospital B's adapter (single hospital, no federation):\")\n",
    "model_b = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16\n",
    ").to(device)\n",
    "model_b = PeftModel.from_pretrained(model_b, \"/content/adapter_hospital_b\")\n",
    "model_b.eval()\n",
    "\n",
    "single_b_results = evaluate_model(\n",
    "    model_b, \"Hospital B only (rank 16)\",\n",
    "    eval_data=eval_examples,\n",
    "    gen_questions=MEDICAL_QUESTIONS,\n",
    ")\n",
    "\n",
    "del model_b\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hospital B Submits → Triggers Aggregation!\n",
    "\n",
    "With `min_deltas=2`, the server will **automatically aggregate** both adapters using FedExLoRA as soon as Hospital B submits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hospital B submits — this triggers aggregation!\n",
    "client_b = ChorusClient(\n",
    "    server=\"http://localhost:8080\",\n",
    "    model_id=MODEL_NAME,\n",
    "    client_id=\"hospital-b\",\n",
    ")\n",
    "\n",
    "result_b = client_b.submit_delta(\n",
    "    adapter_path=\"/content/adapter_hospital_b\",\n",
    "    dataset_size=len(hospital_b_data),\n",
    ")\n",
    "\n",
    "print(f\"Hospital B submitted!\")\n",
    "print(f\"  Round: {result_b['round_id']}\")\n",
    "print(f\"  Deltas received: {result_b['deltas_received']}/{result_b['min_deltas']}\")\n",
    "print(f\"  Aggregated: {result_b['aggregated']}\")\n",
    "\n",
    "if result_b['aggregated']:\n",
    "    print(f\"\\n  FedExLoRA aggregation complete!\")\n",
    "    print(f\"  The server merged rank-8 + rank-16 adapters using SVD-optimal aggregation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Pull Aggregated Adapter from Chorus Server\n",
    "\n",
    "Now we pull the federated model — the merged adapter that contains knowledge from **both** hospitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the aggregated adapter\n",
    "agg_path = client_b.pull_latest(output_path=\"/content/adapter_federated\")\n",
    "client_b.close()\n",
    "\n",
    "print(f\"Aggregated adapter pulled to: {agg_path}\")\n",
    "print(f\"File size: {os.path.getsize(agg_path) / 1e6:.1f} MB\")\n",
    "\n",
    "# Inspect the adapter\n",
    "agg_tensors = load_file(str(agg_path))\n",
    "ranks = set(v.shape[0] for k, v in agg_tensors.items() if \"lora_A\" in k)\n",
    "print(f\"Tensors: {len(agg_tensors)}\")\n",
    "print(f\"Output rank: {ranks} (max of input ranks 8 and 16)\")\n",
    "\n",
    "# Check server status\n",
    "r = requests.get(\"http://localhost:8080/health\")\n",
    "print(f\"\\nServer health: {r.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate the Federated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FEDERATED MODEL: FedExLoRA aggregation of both hospitals\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fed_model = load_adapter_into_model(agg_tensors, rank=max(ranks))\n",
    "\n",
    "federated_results = evaluate_model(\n",
    "    fed_model, \"Federated (FedExLoRA)\",\n",
    "    eval_data=eval_examples,\n",
    "    gen_questions=MEDICAL_QUESTIONS,\n",
    ")\n",
    "\n",
    "del fed_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Comparison\n",
    "\n",
    "The moment of truth — did federated learning actually help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTS: Medical QA Perplexity (lower = better)\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "all_results = [\n",
    "    baseline_results,\n",
    "    single_a_results,\n",
    "    single_b_results,\n",
    "    federated_results,\n",
    "]\n",
    "\n",
    "# Find best perplexity for highlighting\n",
    "best_ppl = min(r[\"perplexity\"] for r in all_results)\n",
    "\n",
    "for r in all_results:\n",
    "    ppl = r[\"perplexity\"]\n",
    "    bar_len = int(ppl / max(rr[\"perplexity\"] for rr in all_results) * 30)\n",
    "    bar = \"█\" * bar_len\n",
    "    marker = \" ◄ BEST\" if ppl == best_ppl else \"\"\n",
    "    imp = \"\"\n",
    "    if r[\"label\"] != \"Base Model\":\n",
    "        pct = (baseline_results[\"perplexity\"] - ppl) / baseline_results[\"perplexity\"] * 100\n",
    "        imp = f\"  ({pct:+.1f}% vs base)\"\n",
    "    print(f\"  {r['label']:30s}  {ppl:8.2f}  {bar}{imp}{marker}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GENERATION COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, q in enumerate(MEDICAL_QUESTIONS):\n",
    "    print(f\"\\n{'─' * 70}\")\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"{'─' * 70}\")\n",
    "    for r in all_results:\n",
    "        if \"generations\" in r:\n",
    "            ans = r[\"generations\"][i][\"a\"][:250]\n",
    "            label = r[\"label\"]\n",
    "            print(f\"\\n  [{label}]:\")\n",
    "            print(f\"  {ans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "  Perplexity (lower = better):\n",
    "    Base model (no training):       {baseline_results['perplexity']:.2f}\n",
    "    Hospital A only (rank 8):       {single_a_results['perplexity']:.2f}\n",
    "    Hospital B only (rank 16):      {single_b_results['perplexity']:.2f}\n",
    "    Federated — FedExLoRA (A + B):  {federated_results['perplexity']:.2f}\n",
    "\n",
    "  What happened:\n",
    "    1. Two hospitals trained LoRA adapters on their PRIVATE medical data\n",
    "    2. They submitted only the small adapters (~4-8 MB) to the Chorus server\n",
    "    3. The server aggregated them using FedExLoRA (exact, heterogeneous ranks)\n",
    "    4. The federated model learned from BOTH hospitals' data\n",
    "    5. Neither hospital ever shared a single patient record\n",
    "\n",
    "  Technical details:\n",
    "    - Hospital A: rank 8, {len(hospital_a_data)} examples\n",
    "    - Hospital B: rank 16, {len(hospital_b_data)} examples\n",
    "    - Aggregated adapter: rank {max(ranks)}, {len(agg_tensors)} tensors\n",
    "    - Aggregation: FedExLoRA (SVD-optimal, mathematically exact)\n",
    "    - Server: Chorus v0.1.0, strategy=fedex-lora\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: stop the Chorus server\n",
    "server_proc.terminate()\n",
    "server_proc.wait(timeout=5)\n",
    "print(\"Chorus server stopped.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
