{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Chorus: Federated Training (Non-IID) — Part 2\n\n**Run `chorus_centralized_baseline.ipynb` in parallel** — it trains the centralized gold standard.\nThis notebook runs the federated loop with non-IID label-skewed clients, then loads centralized\nresults at the end for the full comparison.\n\n| Model | Training Data | Data Distribution | Purpose |\n|-------|--------------|-------------------|----------|\n| **Client-only** (×3) | ~670 each | Non-IID (label skewed) | What each org gets alone |\n| **Federated** (3 rounds) | ~670 each, aggregated | Non-IID (label skewed) | Chorus + FedEx-LoRA |\n\n| Setting | Value |\n|---------|-------|\n| Model | `Qwen/Qwen2.5-0.5B` (490M params, BASE) |\n| Dataset | AG News — 2K train, 500 test, 4 classes |\n| Non-IID | Dirichlet(α=0.3) label skew across 3 clients |\n| LoRA | rank=16, alpha=32, 1 epoch |\n| Aggregation | FedEx-LoRA (SVD-optimal) |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q pyarrow==17.0.0\n!pip install -q --upgrade typing_extensions pydantic pydantic-core\n!pip install -q 'chorus[peft] @ git+https://github.com/varmabudharaju/chorus.git'\n!pip install -q scikit-learn"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch, gc, os, time, json, logging, random\nimport numpy as np\nfrom collections import Counter\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s [%(name)s] %(message)s', datefmt='%H:%M:%S')\n\n# ── Data directory: auto-detect Colab vs SageMaker ───────────────\nif os.path.exists(\"/content\"):\n    DATA_DIR = \"/content/chorus_exp\"        # Colab\nelse:\n    DATA_DIR = os.path.expanduser(\"~/chorus_exp\")  # SageMaker / local\nos.makedirs(DATA_DIR, exist_ok=True)\nprint(f\"Data directory: {DATA_DIR}\")\n\n# ── Hyperparameters (must match centralized notebook) ────────────────\nMODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\nDATASET_SIZE = 2000         # Training examples (from 120K available)\nTEST_SIZE = 500             # Subsample test set for eval\nNUM_CLIENTS = 3\nNUM_ROUNDS = 3\nLORA_RANK = 16\nLORA_ALPHA = 32\nLEARNING_RATE = 3e-4\nNUM_EPOCHS = 1\nBATCH_SIZE = 8\nGRAD_ACCUM = 2\nMAX_SEQ_LEN = 128\nDIRICHLET_ALPHA = 0.3       # Lower = more skewed\nSEED = 42\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\nif device == \"cuda\":\n    gpu = torch.cuda.get_device_properties(0)\n    print(f\"GPU: {gpu.name} | VRAM: {gpu.total_memory / 1e9:.1f} GB\")\nelse:\n    print(\"WARNING: No GPU! Runtime → Change runtime type → T4 GPU\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset — AG News\n",
    "\n",
    "- 4 classes: World (0), Sports (1), Business (2), Sci/Tech (3)\n",
    "- Subsample 6K from 120K training set (same subset as centralized notebook)\n",
    "- Subsample 2K from 7.6K test split for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "LABEL_NAMES = [\"world\", \"sports\", \"business\", \"scitech\"]\n",
    "LABEL_MAP = {0: \"world\", 1: \"sports\", 2: \"business\", 3: \"scitech\"}\n",
    "\n",
    "raw_train = load_dataset(\"ag_news\", split=\"train\").shuffle(seed=SEED)\n",
    "raw_test = load_dataset(\"ag_news\", split=\"test\").shuffle(seed=SEED)\n",
    "\n",
    "train_ds = raw_train.select(range(DATASET_SIZE))\n",
    "test_ds = raw_test.select(range(TEST_SIZE))\n",
    "\n",
    "print(f\"Train: {len(train_ds)} | Test: {len(test_ds)}\")\n",
    "print(f\"Train label distribution: {dict(sorted(Counter(train_ds['label']).items()))}\")\n",
    "print(f\"Test label distribution:  {dict(sorted(Counter(test_ds['label']).items()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Non-IID Split — Dirichlet(α=0.3)\n",
    "\n",
    "Each client gets a skewed subset where some classes dominate.\n",
    "Standard FL benchmark approach (McMahan et al.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def dirichlet_split(dataset, num_clients, alpha, seed=42):\n    \"\"\"Split dataset across clients using Dirichlet distribution for label skew.\"\"\"\n    rng = np.random.default_rng(seed)\n    labels = np.array(dataset[\"label\"])\n    num_classes = len(set(labels))\n    client_indices = [[] for _ in range(num_clients)]\n\n    for c in range(num_classes):\n        class_idx = np.where(labels == c)[0]\n        rng.shuffle(class_idx)\n        proportions = rng.dirichlet(np.repeat(alpha, num_clients))\n        counts = (proportions * len(class_idx)).astype(int)\n        counts[-1] = len(class_idx) - counts[:-1].sum()\n        start = 0\n        for i in range(num_clients):\n            client_indices[i].extend(class_idx[start:start + counts[i]].tolist())\n            start += counts[i]\n\n    for idx_list in client_indices:\n        rng.shuffle(idx_list)\n\n    return [dataset.select(indices) for indices in client_indices]\n\n\nclient_shards = dirichlet_split(train_ds, NUM_CLIENTS, DIRICHLET_ALPHA, seed=SEED)\n\ndef format_example(ex):\n    return f\"Sentence: {ex['text']}\\nCategory: {LABEL_MAP[ex['label']]}\"\n\nprint(f\"{'Client':<10} {'Total':>6} {'World':>7} {'Sports':>7} {'Business':>9} {'SciTech':>8}\")\nprint(\"-\" * 50)\n\nclient_json_paths = []\nfor i, shard in enumerate(client_shards):\n    dist = Counter(shard[\"label\"])\n    print(f\"Client {i:<3} {len(shard):>6} {dist.get(0,0):>7} {dist.get(1,0):>7} {dist.get(2,0):>9} {dist.get(3,0):>8}\")\n\n    texts = [format_example(shard[j]) for j in range(len(shard))]\n    cds = Dataset.from_dict({\"text\": texts})\n    path = f\"{DATA_DIR}/client_{i}.json\"\n    cds.to_json(path)\n    client_json_paths.append(path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model + Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "USE_BF16 = torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False\n",
    "DTYPE = torch.bfloat16 if USE_BF16 else torch.float16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "label_token_ids = {\n",
    "    0: tokenizer.encode(\" world\", add_special_tokens=False)[0],\n",
    "    1: tokenizer.encode(\" sports\", add_special_tokens=False)[0],\n",
    "    2: tokenizer.encode(\" business\", add_special_tokens=False)[0],\n",
    "    3: tokenizer.encode(\" scitech\", add_special_tokens=False)[0],\n",
    "}\n",
    "print(f\"Label tokens: {label_token_ids}\")\n",
    "print(f\"Model: {MODEL_NAME} | dtype: {DTYPE}\")\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, test_data, label):\n",
    "    \"\"\"Evaluate via next-token logit comparison over label tokens.\"\"\"\n",
    "    model.eval()\n",
    "    preds, golds = [], []\n",
    "    for ex in test_data:\n",
    "        prompt = f\"Sentence: {ex['text']}\\nCategory:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LEN).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits[:, -1, :]\n",
    "        scores = {lbl: logits[0, tid].item() for lbl, tid in label_token_ids.items()}\n",
    "        preds.append(max(scores, key=scores.get))\n",
    "        golds.append(ex[\"label\"])\n",
    "    acc = accuracy_score(golds, preds)\n",
    "    report = classification_report(\n",
    "        golds, preds, target_names=LABEL_NAMES,\n",
    "        output_dict=True, zero_division=0\n",
    "    )\n",
    "    print(\n",
    "        f\"  [{label}] Accuracy: {acc:.1%}  |  F1: \"\n",
    "        f\"world={report['world']['f1-score']:.2f} \"\n",
    "        f\"sports={report['sports']['f1-score']:.2f} \"\n",
    "        f\"business={report['business']['f1-score']:.2f} \"\n",
    "        f\"scitech={report['scitech']['f1-score']:.2f}\"\n",
    "    )\n",
    "    return acc, report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Start Chorus Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import requests, threading, socket\n\ndef find_free_port():\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind((\"127.0.0.1\", 0))\n        return s.getsockname()[1]\n\nSERVER_PORT = find_free_port()\nSERVER_URL = f\"http://127.0.0.1:{SERVER_PORT}\"\n\nfrom chorus.server.app import configure, app\nimport uvicorn\n\nconfigure(\n    model_id=MODEL_NAME,\n    data_dir=f\"{DATA_DIR}/chorus_data\",\n    strategy=\"fedex-lora\",\n    min_deltas=NUM_CLIENTS,\n)\n\nserver_error = []\ndef run_server():\n    try:\n        uvicorn.run(app, host=\"127.0.0.1\", port=SERVER_PORT, log_level=\"warning\")\n    except Exception as e:\n        server_error.append(str(e))\n\nthreading.Thread(target=run_server, daemon=True).start()\n\nfor i in range(15):\n    time.sleep(1)\n    if server_error:\n        print(f\"ERROR: {server_error[0]}\")\n        break\n    try:\n        r = requests.get(f\"{SERVER_URL}/health\")\n        if r.status_code == 200:\n            print(f\"Server running at {SERVER_URL} | strategy: {r.json()['strategy']}\")\n            break\n    except:\n        pass"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Federated Training — 3 Rounds × 3 Non-IID Clients\n",
    "\n",
    "Each round:\n",
    "1. All 3 clients train LoRA on their skewed data (starting from aggregated adapter after round 1)\n",
    "2. Submit deltas → server aggregates with FedEx-LoRA\n",
    "3. Pull aggregated adapter → evaluate → use as starting point for next round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from chorus.client.trainer import LoRATrainer\nfrom chorus.client.sdk import ChorusClient\nfrom safetensors.torch import load_file\nfrom peft import PeftModel, LoraConfig, get_peft_model\n\n# PEFT adapter config for loading aggregated adapters\nADAPTER_CONFIG = {\n    \"peft_type\": \"LORA\",\n    \"auto_mapping\": None,\n    \"base_model_name_or_path\": MODEL_NAME,\n    \"bias\": \"none\",\n    \"fan_in_fan_out\": False,\n    \"inference_mode\": True,\n    \"init_lora_weights\": True,\n    \"lora_alpha\": LORA_ALPHA,\n    \"lora_dropout\": 0.0,\n    \"r\": LORA_RANK,\n    \"target_modules\": [\"q_proj\", \"v_proj\"],\n    \"task_type\": \"CAUSAL_LM\",\n}\n\nround_results = []\nadapter_path_for_next_round = None\n\nfor rnd in range(NUM_ROUNDS):\n    print(f\"\\n{'#'*70}\")\n    print(f\"  ROUND {rnd + 1} / {NUM_ROUNDS}\")\n    print(f\"{'#'*70}\")\n\n    round_client_accs = []\n    round_client_reports = []\n\n    for i in range(NUM_CLIENTS):\n        t0 = time.time()\n        print(f\"\\n  --- Client {i} (round {rnd+1}) ---\")\n\n        output_dir = f\"{DATA_DIR}/adapter_r{rnd}_client_{i}\"\n\n        trainer = LoRATrainer(\n            base_model=MODEL_NAME,\n            dataset=client_json_paths[i],\n            output_dir=output_dir,\n            lora_rank=LORA_RANK,\n            lora_alpha=LORA_ALPHA,\n            learning_rate=LEARNING_RATE,\n            num_epochs=NUM_EPOCHS,\n            per_device_batch_size=BATCH_SIZE,\n            gradient_accumulation_steps=GRAD_ACCUM,\n            max_seq_length=MAX_SEQ_LEN,\n            bf16=USE_BF16,\n            fp16=not USE_BF16 and torch.cuda.is_available(),\n            adapter_path=adapter_path_for_next_round,\n            dataloader_pin_memory=False,\n        )\n\n        adapter_path = trainer.train()\n        elapsed = time.time() - t0\n        print(f\"  Trained in {elapsed:.0f}s\")\n\n        # Evaluate individual client\n        single_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(device)\n        single_model = PeftModel.from_pretrained(single_model, output_dir)\n        acc_i, report_i = evaluate_accuracy(single_model, test_ds, f\"R{rnd+1} Client {i}\")\n        round_client_accs.append(acc_i)\n        round_client_reports.append(report_i)\n        del single_model; gc.collect(); torch.cuda.empty_cache()\n\n        # Submit delta\n        client = ChorusClient(server=SERVER_URL, model_id=MODEL_NAME, client_id=f\"client-{i}\")\n        result = client.submit_delta(adapter_path=output_dir, dataset_size=len(client_shards[i]))\n        client.close()\n\n        print(f\"  Submitted: {result['deltas_received']}/{result['min_deltas']}\")\n        if result[\"aggregated\"]:\n            print(f\"  >>> AGGREGATION TRIGGERED <<<\")\n\n    # Pull aggregated adapter\n    pull_dir = f\"{DATA_DIR}/adapter_federated_r{rnd}\"\n    client = ChorusClient(server=SERVER_URL, model_id=MODEL_NAME)\n    agg_path = client.pull_latest(output_path=pull_dir, adapter_config=ADAPTER_CONFIG)\n    client.close()\n\n    # Evaluate federated model\n    agg_tensors = load_file(str(agg_path))\n    ranks = set(v.shape[0] for k, v in agg_tensors.items() if \"lora_A\" in k)\n    agg_rank = max(ranks)\n\n    fed_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=DTYPE).to(device)\n    lora_config = LoraConfig(\n        r=agg_rank, lora_alpha=LORA_ALPHA,\n        target_modules=[\"q_proj\", \"v_proj\"],\n        lora_dropout=0.0, bias=\"none\", task_type=\"CAUSAL_LM\",\n    )\n    fed_model = get_peft_model(fed_model, lora_config)\n\n    state_dict = fed_model.state_dict()\n    for k, v in agg_tensors.items():\n        peft_key = \"base_model.model.\" + k.replace(\n            \".lora_A.weight\", \".lora_A.default.weight\"\n        ).replace(\n            \".lora_B.weight\", \".lora_B.default.weight\"\n        )\n        if peft_key in state_dict:\n            state_dict[peft_key] = v.to(state_dict[peft_key].dtype).to(device)\n    fed_model.load_state_dict(state_dict)\n\n    fed_acc, fed_report = evaluate_accuracy(fed_model, test_ds, f\"Federated R{rnd+1}\")\n    del fed_model; gc.collect(); torch.cuda.empty_cache()\n\n    round_results.append({\n        \"round\": rnd + 1,\n        \"client_accs\": round_client_accs,\n        \"client_reports\": round_client_reports,\n        \"fed_acc\": fed_acc,\n        \"fed_report\": fed_report,\n    })\n\n    adapter_path_for_next_round = pull_dir\n\n    print(f\"\\n  Round {rnd+1} summary: clients={[f'{a:.1%}' for a in round_client_accs]} | federated={fed_acc:.1%}\")\n\nprint(f\"\\n{'#'*70}\")\nprint(\"ALL ROUNDS COMPLETE\")\nprint(f\"{'#'*70}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load Centralized Results\n",
    "\n",
    "Load baseline and centralized results from the parallel notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results_path = f\"{DATA_DIR}/centralized_results.json\"\ntry:\n    with open(results_path) as f:\n        central_results = json.load(f)\n    baseline_acc = central_results[\"baseline_acc\"]\n    baseline_report = central_results[\"baseline_report\"]\n    centralized_acc = central_results[\"centralized_acc\"]\n    centralized_report = central_results[\"centralized_report\"]\n    print(f\"Loaded centralized results from {results_path}\")\n    print(f\"  Baseline:    {baseline_acc:.1%}\")\n    print(f\"  Centralized: {centralized_acc:.1%}\")\nexcept FileNotFoundError:\n    print(f\"ERROR: {results_path} not found!\")\n    print(\"Run chorus_centralized_baseline.ipynb first (or in parallel).\")\n    print(\"Proceeding without centralized comparison...\")\n    baseline_acc = None\n    baseline_report = None\n    centralized_acc = None\n    centralized_report = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 85)\n",
    "print(\"RESULTS: AG News Classification — Centralized vs Federated (Non-IID)\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "header = f\"{'Model':<40} {'Accuracy':>10} {'World F1':>9} {'Sports F1':>10} {'Biz F1':>8} {'Sci F1':>8}\"\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "def pr(label, acc, report):\n",
    "    print(\n",
    "        f\"{label:<40} {acc:>9.1%} \"\n",
    "        f\"{report['world']['f1-score']:>9.2f} \"\n",
    "        f\"{report['sports']['f1-score']:>10.2f} \"\n",
    "        f\"{report['business']['f1-score']:>8.2f} \"\n",
    "        f\"{report['scitech']['f1-score']:>8.2f}\"\n",
    "    )\n",
    "\n",
    "if baseline_acc is not None:\n",
    "    pr(\"Baseline (zero-shot)\", baseline_acc, baseline_report)\n",
    "if centralized_acc is not None:\n",
    "    pr(f\"Centralized (all {DATASET_SIZE}, IID)\", centralized_acc, centralized_report)\n",
    "print()\n",
    "\n",
    "for rr in round_results:\n",
    "    rnd = rr[\"round\"]\n",
    "    for i in range(NUM_CLIENTS):\n",
    "        pr(f\"  Round {rnd} — Client {i}\", rr[\"client_accs\"][i], rr[\"client_reports\"][i])\n",
    "    pr(f\"  Round {rnd} — FEDERATED\", rr[\"fed_acc\"], rr[\"fed_report\"])\n",
    "    avg_c = sum(rr[\"client_accs\"]) / len(rr[\"client_accs\"])\n",
    "    print(f\"  {'':>40} avg client: {avg_c:.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Progression Across Rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"PROGRESSION ACROSS ROUNDS\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "col_w = 12\n",
    "header = f\"{'':>20}\" + \"\".join(f\"{'Round '+str(rr['round']):>{col_w}}\" for rr in round_results)\n",
    "print(f\"\\n{header}\")\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "print(f\"{'Avg client':>20}\", end=\"\")\n",
    "for rr in round_results:\n",
    "    avg = sum(rr[\"client_accs\"]) / len(rr[\"client_accs\"])\n",
    "    print(f\"{avg:>{col_w}.1%}\", end=\"\")\n",
    "print()\n",
    "\n",
    "print(f\"{'Federated':>20}\", end=\"\")\n",
    "for rr in round_results:\n",
    "    print(f\"{rr['fed_acc']:>{col_w}.1%}\", end=\"\")\n",
    "print()\n",
    "\n",
    "if centralized_acc is not None:\n",
    "    print(f\"{'Centralized':>20}\", end=\"\")\n",
    "    for _ in round_results:\n",
    "        print(f\"{centralized_acc:>{col_w}.1%}\", end=\"\")\n",
    "    print(\"  ← target\")\n",
    "\n",
    "if baseline_acc is not None:\n",
    "    print(f\"{'Baseline':>20}\", end=\"\")\n",
    "    for _ in round_results:\n",
    "        print(f\"{baseline_acc:>{col_w}.1%}\", end=\"\")\n",
    "    print(\"  ← floor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "final = round_results[-1]\n",
    "final_fed = final[\"fed_acc\"]\n",
    "final_avg_client = sum(final[\"client_accs\"]) / len(final[\"client_accs\"])\n",
    "final_best_client = max(final[\"client_accs\"])\n",
    "r1_fed = round_results[0][\"fed_acc\"]\n",
    "\n",
    "if baseline_acc is not None:\n",
    "    print(f\"\\nBaseline (zero-shot):  {baseline_acc:.1%}\")\n",
    "if centralized_acc is not None:\n",
    "    print(f\"Centralized (target):  {centralized_acc:.1%}\")\n",
    "print(f\"Federated Round 1:     {r1_fed:.1%}\")\n",
    "print(f\"Federated Round {NUM_ROUNDS}:     {final_fed:.1%}\")\n",
    "print(f\"Avg client (final):    {final_avg_client:.1%}\")\n",
    "print(f\"Best client (final):   {final_best_client:.1%}\")\n",
    "\n",
    "if centralized_acc is not None and baseline_acc is not None:\n",
    "    gap_to_central = centralized_acc - baseline_acc\n",
    "    fed_improvement = final_fed - baseline_acc\n",
    "    if gap_to_central > 0:\n",
    "        recovery = fed_improvement / gap_to_central * 100\n",
    "    else:\n",
    "        recovery = 0\n",
    "    print(f\"\\nFederation recovered {recovery:.0f}% of the centralized gain over baseline\")\n",
    "\n",
    "print(f\"Round 1 → Round {NUM_ROUNDS} improvement: {(final_fed - r1_fed)*100:+.1f} pp\")\n",
    "print(f\"Federation vs avg client: {(final_fed - final_avg_client)*100:+.1f} pp\")\n",
    "\n",
    "if centralized_acc is not None and final_fed >= centralized_acc * 0.95:\n",
    "    print(\"\\n>>> FEDERATION MATCHED CENTRALIZED (within 5%). Non-IID challenge overcome! <<<\")\n",
    "elif final_fed > final_best_client:\n",
    "    print(\"\\n>>> FEDERATION BEAT EVERY CLIENT. Collaboration works even with skewed data. <<<\")\n",
    "elif final_fed > final_avg_client:\n",
    "    print(\"\\n>>> Federation beat average client. Partial success — non-IID hurts but doesn't break it. <<<\")\n",
    "else:\n",
    "    print(\"\\n>>> Federation underperformed. Non-IID skew was too severe. <<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done! Chorus server will shut down when this session ends.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}